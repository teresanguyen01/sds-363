---
title: "S&DS 363 Pset1"
output: html_document
date: "2025-01-28"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
#This chunk defines several helpful functions

#n is the number of observations in the dataset
#p is the number of variables in the dataset

parallel<-function(n,p){
  
  if (n > 1000 || p > 100) {
    print ("Sorry, this only works for n<1000 and p<100")
    stop()
  }
  
  coefs <- matrix(
    c(0.0316, 0.7611, -0.0979, -0.3138, 0.9794, -.2059, .1226, 0, 0.1162, 
      0.8613, -0.1122, -0.9281, -0.3781, 0.0461, 0.0040, 1.0578, 0.1835, 
      0.9436, -0.1237, -1.4173, -0.3306, 0.0424, .0003, 1.0805 , 0.2578, 
      1.0636, -0.1388, -1.9976, -0.2795, 0.0364, -.0003, 1.0714, 0.3171, 
      1.1370, -0.1494, -2.4200, -0.2670, 0.0360, -.0024, 1.08994, 0.3809, 
      1.2213, -0.1619, -2.8644, -0.2632, 0.0368, -.0040, 1.1039, 0.4492, 
      1.3111, -0.1751, -3.3392, -0.2580, 0.0360, -.0039, 1.1173, 0.5309, 
      1.4265, -0.1925, -3.8950, -0.2544, 0.0373, -.0064, 1.1421, 0.5734, 
      1.4818, -0.1986, -4.2420, -0.2111, 0.0329, -.0079, 1.1229, 0.6460, 
      1.5802, -0.2134, -4.7384, -0.1964, 0.0310, -.0083, 1.1320),ncol=8, byrow=TRUE)
  
  calclim <- p
  if (p > 10) calclim <- 10
  coefsred <- coefs[1:calclim, ]
  temp <- c(p:1)
  #stick <- sort(cumsum(1/temp), decreasing=TRUE)[1:calclim]
  multipliers <- matrix(c(log(n),log(p),log(n)*log(p),1), nrow=1)
  longman <- exp(multipliers%*%t(coefs[,1:4]))[1:calclim]
  allen <- rep(NA, calclim)
  leig0 <- 0
  newlim <- calclim
  if (calclim+2 < p) newlim <-newlim+2
  for (i in 1:(newlim-2)){
    leig1 <- coefsred[i,5:8]%*%matrix(c(1,log(n-1),log((p-i-1)*(p-i+2)/2), leig0))
    leig0 <- leig1
    allen[i] <- exp(leig1)
  }
  pcompnum <- c(1:calclim)
  #data.frame(cbind(pcompnum,stick,longman,allen))
  data.frame(cbind(pcompnum,longman,allen))  
}

#########
#this function makes a nice plot if given the input from a PCA analysis
#created by prcomp()
##
#arguments are
#    n=number of observations

parallelplot <- function(comp){
  if (dim(comp$x)[1] > 1000 || length(comp$sdev) > 100) {
    print ("Sorry, this only works for n < 1000 and p < 100")
    stop()
  }
  #if (round(length(comp$sdev)) < round(sum(comp$sdev^2))) {
  #    print ("Sorry, this only works for analyses using the correlation matrix")
  #    stop()
  # }
  
  parallelanal <- parallel(dim(comp$x)[1], length(comp$sdev))
  print(parallelanal)
  calclim <- min(10, length(comp$sdev))
  eigenvalues <- (comp$sdev^2)[1:calclim]
  limits <- as.matrix(parallelanal[,2:3])
  limits <- limits[complete.cases(limits)]
  ymax <- range(c(eigenvalues),limits)
  plot(parallelanal$pcompnum, eigenvalues, xlab="Principal Component Number",
       ylim=c(ymax), ylab="Eigenvalues and Thresholds",
       main="Scree Plot with Parallel Analysis Limits",type="b",pch=15,lwd=2, col="red")
  #lines(parallelanal$pcompnum,parallelanal[,2], type="b",col="red",pch=16,lwd=2)
  lines(parallelanal$pcompnum,parallelanal[,2], type="b",col="green",pch=17,lwd=2)
  lines(parallelanal$pcompnum,parallelanal[,3], type="b",col="blue",pch=18,lwd=2)
  #legend((calclim/2),ymax[2],legend=c("Eigenvalues","Stick Method","Longman Method","Allen Method"),  pch=c(15:18), col=c("black","red","green","blue"),lwd=2)
  legend((calclim/2), ymax[2], legend=c("Eigenvalues","Longman Method","Allen Method"),  pch = c(16:18), col= c("red","green","blue"), lwd=2)
}


#make score plot with confidence ellipse.
#arguments are output from prcomp, vector with components for plotting (usually c(1,2) or c(1,3)
#and a vector of names for the points

ciscoreplot<-function(x, comps, namevec){
  y1<-sqrt(5.99*(x$sdev[comps[1]]^2))
  ymod<-y1-y1%%.05
  y1vec<-c(-y1,seq(-ymod,ymod,by=0.05),y1)
  y2vecpos<-sqrt((5.99-(y1vec^2)/x$sdev[comps[1]]^2)*x$sdev[comps[2]]^2)
  y2vecneg<--sqrt((5.99-(y1vec^2)/x$sdev[comps[1]]^2)*x$sdev[comps[2]]^2)
  y2vecpos[1]<-0
  y2vecneg[1]<-0
  y2vecpos[length(y2vecpos)]<-0
  y2vecneg[length(y2vecneg)]<-0
  
  plot(x$x[,comps[1]],x$x[,comps[2]], 
       pch = 19, 
       cex = 1.2,
       xlim = c(min(y1vec, x$x[, comps[1]]), max(y1vec, x$x[, comps[1]])),
       ylim = c(min(y2vecneg, x$x[, comps[2]]), max(y2vecpos, x$x[, comps[2]])),
       main = "PC Score Plot with 95% CI Ellipse", 
       xlab = paste("Scores for PC", comps[1], sep = " "), 
       ylab = paste("Scores for PC", comps[2], sep = " "))
  
  lines(y1vec,y2vecpos,col="Red",lwd=2)
  lines(y1vec,y2vecneg,col="Red",lwd=2)
  outliers<-((x$x[,comps[1]]^2)/(x$sdev[comps[1]]^2)+(x$x[,comps[2]]^2)/(x$sdev[comps[2]]^2))>5.99
  
  points(x$x[outliers, comps[1]], x$x[outliers, comps[2]], pch = 19, cex = 1.2, col = "Blue")
  
  text(x$x[outliers, comps[1]],x$x[outliers, comps[2]], col = "Blue", lab = namevec[outliers])
}


```



```{r cars}
data <- read.csv("../../Documents/sds-363/Human Development Index - Full.csv")
data <- data[complete.cases(data), ]
dim(data)

# Rename the columns based on the associated goal
colnames(data) <- c(
  "country_code",                         # Country code of the respective country
  "country",                              # Name of the country
  "region",                               # Region to which the country belongs
  "overall_score",                        # Overall SDG index score
  "no_poverty",                           # Goal 1: No Poverty
  "zero_hunger",                          # Goal 2: Zero Hunger
  "good_health_and_wellbeing",            # Goal 3: Good Health and Wellbeing
  "quality_education",                    # Goal 4: Quality Education
  "gender_equality",                      # Goal 5: Gender Equality
  "clean_water_and_sanitation",           # Goal 6: Clean Water and Sanitation
  "affordable_and_clean_energy",          # Goal 7: Affordable and Clean Energy
  "decent_work_and_economic_growth",      # Goal 8: Decent Work and Economic Growth
  "industry_innovation_and_infrastructure", # Goal 9: Industry, Innovation and Infrastructure
  "reduced_inequalities",                 # Goal 10: Reduced Inequalities
  "sustainable_cities_and_communities",   # Goal 11: Sustainable Cities and Communities
  "responsible_consumption_and_production", # Goal 12: Responsible Consumption and Production
  "climate_action",                       # Goal 13: Climate Action
  "life_below_water",                     # Goal 14: Life Below Water
  "life_on_land",                         # Goal 15: Life on Land
  "peace_justice_and_strong_institutions",# Goal 16: Peace, Justice and Strong Institutions
  "partnerships_for_the_goals"            # Goal 17: Partnerships for the Goals
)

#change overall score to a categorical variable (70 is high and 50 is low)
data$overall_score <- ifelse(
  data$overall_score < 50, "Poor",
  ifelse(data$overall_score > 70, "Good", "Average")
)

# Convert the new column to a factor
data$overall_score <- as.factor(data$overall_score)


# Check dataset
head(data)
table(data$overall_score)

```
```{r, warning=FALSE}
library(corrplot)

#include complete cases
data <- data[complete.cases(data), ]
data <- data[, c(
  "Country",
  "UNDP.Developing.Regions",
  "HDI.Rank..2021.",
  "Human.Development.Index..2021.",
  "Life.Expectancy.at.Birth..2021.",
  "Mean.Years.of.Schooling..2021.",
  "Gross.National.Income.Per.Capita..2021.",
  "Gender.Development.Index..2021.",
  "Coefficient.of.human.inequality..2021.",
  "Overall.loss......2021.",
  "Gender.Inequality.Index..2021.",
  "Maternal.Mortality.Ratio..deaths.per.100.000.live.births...2021.",
  "Adolescent.Birth.Rate..births.per.1.000.women.ages.15.19...2021.",
  "Labour.force.participation.rate..female....ages.15.and.older...2021.",
  "Labour.force.participation.rate..male....ages.15.and.older...2021.",
  "Carbon.dioxide.emissions.per.capita..production...tonnes...2021."
)]

colnames(data) <- c(
  "Country",
  "Region",
  "HDI_Rank_2021",
  "HDI_2021",
  "Life_Expectancy_2021",
  "Mean_Years_Schooling_2021",
  "GNI_Per_Capita_2021",
  "Gender_Dev_Index_2021",
  "Human_Inequality_Coeff_2021",
  "Overall_Loss_2021",
  "Gender_Inequality_Index_2021",
  "Maternal_Mortality_Rate_2021",
  "Adolescent_Birth_Rate_2021",
  "Female_Labour_Force_2021",
  "Male_Labour_Force_2021",
  "CO2_Emissions_percapita_2021"
)

missing_counts <- colSums(is.na(data))
missing_counts
most_missing_column <- names(missing_counts[which.max(missing_counts)])
most_missing_column


dim(data)





corrplot.mixed(cor(data[ ,-c(1:4)]), lower.col = "black", upper = "ellipse", tl.col = "black", number.cex = .3, order = "hclust", tl.pos = "lt", tl.cex = .5)


chart.Correlation(data[, -c(1:4)])
```
*Based on our plots showing the correlation between our variables, there appears to be decently strong positive and negative correlations between our variables, suggesting that PCA may be an effective strategy to utilize since we can reduce dimensionality without losing too much information. Moreover, the relationships exhibited between variables, for the most part, appear to be linear. Looking at the scatter plots between our variables does show some non-linearity, which may suggest the use of transformations.*

```{r}
# MAKE chi-square quantile plot
library(heplots)
cqplot(data[, -c(1:4)], main = "World Bank Data")
```

```{r}
#TRANSFORMATIONS NOT NECESSARY
# Apply log transformation to specific columns 
data[c("no_poverty", "quality_education", "decent_work_and_economic_growth", "sustainable_cities_and_communities", "responsible_consumption_and_production", "climate_action")] <- lapply(data[c("no_poverty", "quality_education", "decent_work_and_economic_growth", "sustainable_cities_and_communities", "responsible_consumption_and_production", "climate_action")], log)

#head(data)
hist(data$no_poverty)
#chart.Correlation(data[, -c(1:4)])
```

```{r}
corrplot(cor(data[ ,-c(1:4)]), method="number", order="FPC", tl.cex = .5, number.cex = .3)
corrplot(cor(data[ ,-c(1:4)]),method = "ellipse", order="FPC", tl.cex = .5)

dim(data)
```
*As mentioned in question 1, there appears to be fairly strong correlations (both positive and negative) between our variables. Based on the corrplot, some variables with very strong positive correlations include community sustainability and health & well being (r = 0.85), quality education and health & well being (r = 0.84), and no poverty and innovation/infrastructure (r = 0.8). Likewise, there are some decently strong negative correlation between our variables such as sustainable cities and responsible consumption (r = -0.72) and climate action and innovation/infrastructure (-0.76). This suggests that PCA should work relatively well on our given data since strong correlations implies that a lot of the variance is being described by multiple variables and as such, we can capture a lot of the variability by using a set of principle components (thus reducing our dimensions overall). Our dataset is composed of 107 complete observations and 17 variables (there are 21 total variables but 4 of them are categorical and left out of our analysis). Relative to the number of variables our sample size is sufficiently large enough (about 6 times the number of observatiosn as variables) and we can use a rule of them to ensure that our dataset is large enough for PCA.*

```{r}
summary.PCA.JDRS <- function(x){
  sum_JDRS <- summary(x)$importance
  sum_JDRS[1, ] <- sum_JDRS[1, ]^2
  attr(sum_JDRS, "dimnames")[[1]][1] <- "Eigenvals (Variance)"
  sum_JDRS
}

pc1 <- prcomp(data[, -c(1:4)], scale. = TRUE)

round(summary.PCA.JDRS(pc1),2)

round(pc1$rotation,2)

screeplot(pc1, type = "lines", col = "red", lwd = 2, pch = 19, cex = 1.2, 
          main = "Scree Plot of Raw WB Data")

source("http://reuningscherer.net/multivariate/r/parallel.R.txt")
parallelplot(pc1)
```


