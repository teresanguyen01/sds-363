---
title: 'S&DS 363 Problem Set 5: Cluster Analysis'
author: "Franklin Wu and Teresa Nguyen"
date: "2025-03-31"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Human Development Index Cluster Analysis

Cluster analysis is a technique used to group similar observations or variables based on shared characteristics. In this assignment, we’ll apply cluster analysis to the Human Development Index (HDI) to identify which countries or regions are most alike in terms of development indicators.

### Data Cleaning and Useful Packages

```{r}
library(aplpack)
library(fpc)
library(cluster)
library(ape)
library(amap)
library(PerformanceAnalytics)
```

Always need to start with this first. Read in Data, choose the columns that best fit our needs, and rename the columns.

```{r}
data <- read.csv("../../Documents/sds-363/Human Development Index - Full.csv")

# Based on going through the Kaggle dataset and choosing the most relevant features
data <- data[, c(
  "Country",
  "UNDP.Developing.Regions",
  "HDI.Rank..2021.",
  "Human.Development.Index..2021.",
  "Life.Expectancy.at.Birth..2021.",
  "Mean.Years.of.Schooling..2021.",
  "Gross.National.Income.Per.Capita..2021.",
  "Gender.Development.Index..2021.",
  "Coefficient.of.human.inequality..2021.",
  "Overall.loss......2021.",
  "Gender.Inequality.Index..2021.",
  "Maternal.Mortality.Ratio..deaths.per.100.000.live.births...2021.",
  "Adolescent.Birth.Rate..births.per.1.000.women.ages.15.19...2021.",
  "Labour.force.participation.rate..female....ages.15.and.older...2021.",
  "Labour.force.participation.rate..male....ages.15.and.older...2021.",
  "Carbon.dioxide.emissions.per.capita..production...tonnes...2021."
)]

# rename for better readability
colnames(data) <- c(
  "Country",
  "Region",
  "HDI_Rank_2021",
  "HDI_2021",
  "Life_Expectancy_2021",
  "Mean_Years_Schooling_2021",
  "GNI_Per_Capita_2021",
  "Gender_Dev_Index_2021",
  "Human_Inequality_Coeff_2021",
  "Overall_Loss_2021",
  "Gender_Inequality_Index_2021",
  "Maternal_Mortality_Rate_2021",
  "Adolescent_Birth_Rate_2021",
  "Female_Labour_Force_2021",
  "Male_Labour_Force_2021",
  "CO2_Emissions_percapita_2021"
)

# our data frame should only include complete cases 
data <- data[complete.cases(data), ]

```

### Distance Metrics & Possible Transformations

```{r}
# Apply log transformation to specific columns 
data[c("GNI_Per_Capita_2021", "Gender_Dev_Index_2021", 
       "Maternal_Mortality_Rate_2021", "Adolescent_Birth_Rate_2021", 
       "CO2_Emissions_percapita_2021")] <- lapply(data[c("GNI_Per_Capita_2021", 
                                                         "Gender_Dev_Index_2021",
                                                         "Maternal_Mortality_Rate_2021",
                                                         "Adolescent_Birth_Rate_2021", 
                                                         "CO2_Emissions_percapita_2021")], log)

chart.Correlation(data[, -c(1:3)])

#one way to standardize data
wbnorm <- data[, c("HDI_2021", "Life_Expectancy_2021", 
                   "Mean_Years_Schooling_2021", "GNI_Per_Capita_2021", 
                   "Gender_Dev_Index_2021", "Gender_Inequality_Index_2021", 
                   "Maternal_Mortality_Rate_2021", "Adolescent_Birth_Rate_2021", 
                   "Female_Labour_Force_2021", "Male_Labour_Force_2021", 
                   "CO2_Emissions_percapita_2021")]
rownames(wbnorm) <- data[, 1]
wbnorm <- scale(na.omit(wbnorm))
dim(wbnorm)

```

We chose Euclidean distance as the most appropriate metric because our data is continuous and standardized, making it suitable for measuring straight-line distances between countries based on various HDI indicators.

Other methods of distance do not fit the context of our data. For instance, our data is not binary so we would not use Jaccard Distance and we are not looking at absolute differences so Manhattan Distance would not be satisfactory. 

To ensure Euclidean distance performs accurately, we standardized our data and made minor preprocessing adjustments, such as setting country names as row labels. Moreover, we applied log transformations to variables with significant skewness to improve the overall distribution and clustering performance.
 
### Hierarchical Cluster Analysis - Euclidean and Manhattan

```{r}
#Euclidean and complete
dist1 <- dist(wbnorm, method = "euclidean")
clust1 <- hclust(dist1)
plot(clust1, labels = rownames(wbnorm), cex = 0.3, xlab = "", 
     ylab = "Distance", main = "Clustering of Countries")
rect.hclust(clust1, k = 3)

#Ward with Manhattan
dist2 <- dist(wbnorm, method = "manhattan")
clust2 <- hclust(dist2, method = "ward.D")
plot(clust2, labels = rownames(wbnorm), cex = 0.6, xlab = "", 
     ylab = "Distance", main = "Clustering of Countries")
rect.hclust(clust2, k = 5)

#Maximum with Single
dist3 <- dist(wbnorm, method = "maximum")
clust3 <- hclust(dist3, method = "single")
plot(clust3, labels = rownames(wbnorm), cex = 0.6, xlab = "", 
     ylab = "Distance", main = "Clustering of Countries")
rect.hclust(clust3, k = 2)
```

The first dendrogram uses *Euclidean distance with complete agglomeration* (also known as the farthest-neighbor method), which merges clusters based on the maximum distance between observations. This approach produced roughly three clusters, with Yemen standing out as a clear outlier forming its own cluster, while the remaining countries split into two relatively compact groups. 

In contrast, the dendrogram using Manhattan distance with Ward’s method—which minimizes the total within-cluster variance—resulted in about five distinct, tightly grouped clusters with lower internal variability.

Lastly, the third method applies Euclidean distance with single linkage (nearest-neighbor), where clusters are formed based on the smallest distance between points. This approach yielded a very different structure: Yemen again appears as a singular outlier, while all other countries are grouped into one large, less structured cluster.

### Number of Groups

```{r}
source("https://raw.githubusercontent.com/jreuning/sds363_code/refs/heads/main/HClusEval3.R.txt")
#Call the function
hclus_eval(wbnorm, dist_m = 'euclidean', clus_m = 'complete', plot_op = T, 
           print_num = 15)
```

To determine the optimal number of clusters, we looked at many metrics. The total sum of squares (RSQ), represented by the red line, demonstrates diminishing returns after the fourth cluster, suggesting limited additional explanatory power beyond that point. The semi-partial R-squared (green line) shows a clear elbow at the third index, indicating a potential grouping into three clusters. The Root Mean Square Standard Deviation (RMSSTD) curve flattens between the fourth and fifth index, pointing toward reduced improvement in within-cluster homogeneity beyond that range. Additionally, the cluster distance plot shows a noticeable elbow around the fourth, and possibly sixth, index. Taking all these indicators into account, four clusters appear to be the most appropriate number to retain.

### K-Means Clustering

```{r}
#kdata is just normalized input dataset
kdata <- wbnorm
n.lev <- 15  #set max value for number of clusters k

# Calculate the within groups sum of squared error (SSE) for the number of 
# solutions selected by the user
wss <- rnorm(10)
while (prod(wss==sort(wss,decreasing=T))==0) {
  wss <- (nrow(kdata)-1)*sum(apply(kdata,2,var))
  for (i in 2:n.lev) wss[i] <- sum(kmeans(kdata, centers=i)$withinss)}

# Calculate the within groups SSE for 250 randomized data sets 
# (based on the original input data)
k.rand <- function(x){
  km.rand <- matrix(sample(x),dim(x)[1],dim(x)[2])
  rand.wss <- as.matrix(dim(x)[1]-1)*sum(apply(km.rand,2,var))
  for (i in 2:n.lev) rand.wss[i] <- sum(kmeans(km.rand, centers=i)$withinss)
  rand.wss <- as.matrix(rand.wss)
  return(rand.wss)
}

rand.mat <- matrix(0,n.lev,250)

k.1 <- function(x) { 
  for (i in 1:250) {
    r.mat <- as.matrix(suppressWarnings(k.rand(kdata)))
    rand.mat[,i] <- r.mat}
  return(rand.mat)
}

# Same function as above for data with < 3 column variables
k.2.rand <- function(x){
  rand.mat <- matrix(0,n.lev,250)
  km.rand <- matrix(sample(x),dim(x)[1],dim(x)[2])
  rand.wss <- as.matrix(dim(x)[1]-1)*sum(apply(km.rand,2,var))
  for (i in 2:n.lev) rand.wss[i] <- sum(kmeans(km.rand, centers=i)$withinss)
  rand.wss <- as.matrix(rand.wss)
  return(rand.wss)
}

k.2 <- function(x){
  for (i in 1:250) {
    r.1 <- k.2.rand(kdata)
    rand.mat[,i] <- r.1}
  return(rand.mat)
}

# Determine if the data data table has > or < 3 variables and call appropriate 
# function above
if (dim(kdata)[2] == 2) { rand.mat <- k.2(kdata) } else { rand.mat <- k.1(kdata) }

# Plot within groups SSE against all tested cluster solutions for actual and 
# randomized data - 1st: Log scale, 2nd: Normal scale

xrange <- range(1:n.lev)
yrange <- range(log(rand.mat),log(wss))
plot(xrange,yrange, type='n', xlab='Cluster Solution', 
     ylab='Log of Within Group SSE', main='Cluster Solutions against Log of SSE')
for (i in 1:250) lines(log(rand.mat[,i]),type='l',col='red')
lines(log(wss), type="b", col='blue')
legend('topright',c('Actual Data', '250 Random Runs'), col=c('blue', 'red'), lty=1)

yrange <- range(rand.mat,wss)
plot(xrange,yrange, type='n', xlab="Cluster Solution", 
     ylab="Within Groups SSE", main="Cluster Solutions against SSE")
for (i in 1:250) lines(rand.mat[,i],type='l',col='red')
lines(1:n.lev, wss, type="b", col='blue')
legend('topright',c('Actual Data', '250 Random Runs'), col=c('blue', 'red'), lty=1)

# Calculate the mean and standard deviation of difference 
# between SSE of actual data and SSE of 250 randomized datasets
r.sse <- matrix(0,dim(rand.mat)[1],dim(rand.mat)[2])
wss.1 <- as.matrix(wss)
for (i in 1:dim(r.sse)[2]) {
  r.temp <- abs(rand.mat[,i]-wss.1[,1])
  r.sse[,i] <- r.temp}
r.sse.m <- apply(r.sse,1,mean)
r.sse.sd <- apply(r.sse,1,sd)
r.sse.plus <- r.sse.m + r.sse.sd
r.sse.min <- r.sse.m - r.sse.sd

# Plot differeince between actual SSE mean SSE from 250 randomized datasets - 
# 1st: Log scale, 2nd: Normal scale 

xrange <- range(1:n.lev)
if (min(r.sse.min) < 0){
   yrange <- range(log(r.sse.plus - min(r.sse.min)*1.05), log(r.sse.min - min(r.sse.min)*1.05))
} else {
   yrange <- range(log(r.sse.plus), log(r.sse.min))
}

plot(xrange,yrange, type='n',xlab='Cluster Solution', 
     ylab='Log of SSE - Random SSE', 
     main='Cluster Solustions against (Log of SSE - Random SSE)')
lines(log(r.sse.m), type="b", col='blue')
lines(log(r.sse.plus), type='l', col='red')
lines(log(r.sse.min), type='l', col='red')
legend('topright',c('SSE - random SSE', 'SD of SSE-random SSE'), col=c('blue', 'red'), lty=1)

xrange <- range(1:n.lev)
yrange <- range(r.sse.plus,r.sse.min)
plot(xrange,yrange, type='n',
     xlab='Cluster Solution', ylab='SSE - Random SSE', 
     main='Cluster Solutions against (SSE - Random SSE)')
lines(r.sse.m, type="b", col='blue')
lines(r.sse.plus, type='l', col='red')
lines(r.sse.min, type='l', col='red')
legend('topright',c('SSE - random SSE', 'SD of SSE-random SSE'), 
       col = c('blue', 'red'), lty = 1)

# Ask for user input - Select the appropriate number of clusters
#choose.clust <- function(){readline("What clustering solution would you like to use? ")} 
#clust.level <- as.integer(choose.clust())
clust.level <- 4

# Apply K-means cluster solutions - append clusters to CSV file
fit <- kmeans(kdata, clust.level)
aggregate(kdata, by=list(fit$cluster), FUN=mean)
clust.out <- fit$cluster
kclust <- as.matrix(clust.out)
kclust.out <- cbind(kclust, wbnorm)
write.table(kclust.out, file="kmeans_out.csv", sep=",")

# Display Principal Components plot of data with clusters identified

clusplot(kdata, fit$cluster, shade = F, labels = 2, lines = 0, color = T,
         lty = 4, main = 'Principal Components plot showing K-means clusters')


#Make plot of five cluster solution in space desginated by first two
#  two discriminant functions

plotcluster(kdata, fit$cluster, main="Four Cluster Solution in DA Space",
            xlab="First Discriminant Function", ylab="Second Discriminant Function")

# end of script
```

After performing K-means clustering, we generated a plot of the total sum of squares versus the number of clusters (k). The elbow in the curve appears around the fourth cluster, suggesting that four is an appropriate number of groups to retain. 

Additionally, we included a plot of the sum of squared errors (SSE) for random permutations of the data (in red), which also shows a subtle kink around the fourth cluster. A plot of the log-transformed SSE similarly points to a maximum around the fourth index. 

Taken together, these plots indicate that four clusters best capture the underlying structure of the data, which aligns with our findings from hierarchical clustering, further strengthening the presence of four naturally distinct groupings within the dataset. 

To visualize these groupings, we plotted the K-means clusters in principal component space (using the first and second components), as well as in discriminant analysis space, both of which show clear separation among the identified clusters.

### Plots in DA and PCA Space

```{r}
clust1 <- hclust(dist1, method = "ward.D")
cuts <- cutree(clust1, k = 4)
clusplot(wbnorm, cuts, color = TRUE, shade = TRUE, labels = 2, lines = 0, 
         main = "World Bank Five Cluster Plot, Complete Linkage, First two PC", 
         cex = .5)
plotcluster(wbnorm, cuts, main = "Five Cluster Solution in DA Space", 
            xlab = "First Discriminant Function", 
            ylab = "Second Discriminant Function", cex = .8)
```
The cluster plots in both discriminant analysis (DA) space and principal component analysis (PCA) space—based on Euclidean distance and Ward’s method—show a clear separation among the groups. Similar with our K-means results, both visualizations suggest the presence of four distinct clusters. These plots' confirmation supports our earlier selection that four is the optimal number of groups to retain.

```{r}
for (i in 1:4){
  print(paste("Countries in Cluster ", i))
  print(rownames(wbnorm)[cuts == i])
  print (" ")
}

pca <- prcomp(wbnorm, scale = TRUE)
pca$rotation  # This shows the loadings

```

Cluster 1 (e.g., Uganda, Nigeria, Sudan) consists of countries with low Human Development Index (HDI) indicators. These countries are positioned on the far left of the first principal component, which appears to represent a general measure of overall HDI performance. 

In contrast, Cluster 4 (e.g., Denmark, Finland, Japan) includes highly developed countries with strong HDI metrics, scoring highest along the same component. The remaining two clusters are less clear. Cluster 2, which includes countries like China, seems to represent nations in transition—those currently progressing toward developed status—positioned between the low and high extremes along the first principal component. 

Cluster 3 appears to contain less developed countries that vary more along the second principal component, which seems to show gender-related development factors, given the strong loadings of variables like male/female labor force participation and the Gender Development Index. 

Overall, the clustering results fit the context of our dataset, effectively grouping countries by their development stage—from low HDI to highly developed, with transitional and intermediate categories in between.
