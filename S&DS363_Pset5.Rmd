---
title: "S&DS363_Pset5"
output: html_document
date: "2025-03-25"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Data Cleaning and Useful Packages

```{r}
library(aplpack)
library(fpc)
library(cluster)
library(ape)
library(amap)
library(PerformanceAnalytics)
```

Always need to start with this first. Read in Data, choose the columns that best fit our needs, and rename the columns.

```{r}
data <- read.csv("../../Documents/sds-363/Human Development Index - Full.csv")

# Based on going through the Kaggle dataset and choosing the most relevant features
data <- data[, c(
  "Country",
  "UNDP.Developing.Regions",
  "HDI.Rank..2021.",
  "Human.Development.Index..2021.",
  "Life.Expectancy.at.Birth..2021.",
  "Mean.Years.of.Schooling..2021.",
  "Gross.National.Income.Per.Capita..2021.",
  "Gender.Development.Index..2021.",
  "Coefficient.of.human.inequality..2021.",
  "Overall.loss......2021.",
  "Gender.Inequality.Index..2021.",
  "Maternal.Mortality.Ratio..deaths.per.100.000.live.births...2021.",
  "Adolescent.Birth.Rate..births.per.1.000.women.ages.15.19...2021.",
  "Labour.force.participation.rate..female....ages.15.and.older...2021.",
  "Labour.force.participation.rate..male....ages.15.and.older...2021.",
  "Carbon.dioxide.emissions.per.capita..production...tonnes...2021."
)]

# rename for better readability
colnames(data) <- c(
  "Country",
  "Region",
  "HDI_Rank_2021",
  "HDI_2021",
  "Life_Expectancy_2021",
  "Mean_Years_Schooling_2021",
  "GNI_Per_Capita_2021",
  "Gender_Dev_Index_2021",
  "Human_Inequality_Coeff_2021",
  "Overall_Loss_2021",
  "Gender_Inequality_Index_2021",
  "Maternal_Mortality_Rate_2021",
  "Adolescent_Birth_Rate_2021",
  "Female_Labour_Force_2021",
  "Male_Labour_Force_2021",
  "CO2_Emissions_percapita_2021"
)

# our data frame should only include complete cases 
data <- data[complete.cases(data), ]

```

```{r}
# Apply log transformation to specific columns 
data[c("GNI_Per_Capita_2021", "Gender_Dev_Index_2021", "Maternal_Mortality_Rate_2021", "Adolescent_Birth_Rate_2021", "CO2_Emissions_percapita_2021")] <- lapply(data[c("GNI_Per_Capita_2021", "Gender_Dev_Index_2021", "Maternal_Mortality_Rate_2021", "Adolescent_Birth_Rate_2021", "CO2_Emissions_percapita_2021")], log)

chart.Correlation(data[, -c(1:3)])

#one way to standardize data
wbnorm <- data[, c("HDI_2021", "Life_Expectancy_2021", "Mean_Years_Schooling_2021", "GNI_Per_Capita_2021", "Gender_Dev_Index_2021", "Gender_Inequality_Index_2021", "Maternal_Mortality_Rate_2021", "Adolescent_Birth_Rate_2021", "Female_Labour_Force_2021", "Male_Labour_Force_2021", "CO2_Emissions_percapita_2021")]
rownames(wbnorm) <- data[, 1]
wbnorm <- scale(na.omit(wbnorm))
dim(wbnorm)

```

*Using euclidean distance is the most appropriate metric to use considering our data is continuous and standardized - we can measure the straight line distance between countries in relation to the various HDI metrics from our data. Other methods of distance do not fit the context of our data. For instance, our data is not binary so we would not use Jaccard Distance and we are not looking at absolute differences so Manhattan Distance would not be satisfactory. Furthermore, since euclidean distance works on standarized data, we must standardize our data - we also made small adjustments such as moving the country column to be the row names. Looking at the distribution of some of our variables, we also made log transformations to adjust for large rightward/leftward skew in the data.*


```{r}
#Euclidean and complete
dist1 <- dist(wbnorm, method = "euclidean")
clust1 <- hclust(dist1)
plot(clust1, labels = rownames(wbnorm), cex = 0.3, xlab = "", ylab = "Distance", main = "Clustering of Countries")
rect.hclust(clust1, k = 3)

#Ward with Manhattan
dist2 <- dist(wbnorm, method = "manhattan")
clust2 <- hclust(dist2, method = "ward.D")
plot(clust2, labels = rownames(wbnorm), cex = 0.6, xlab = "", ylab = "Distance", main = "Clustering of Countries")
rect.hclust(clust2, k = 5)

#Maximum with Single
dist3 <- dist(wbnorm, method = "maximum")
clust3 <- hclust(dist3, method = "single")
plot(clust3, labels = rownames(wbnorm), cex = 0.6, xlab = "", ylab = "Distance", main = "Clustering of Countries")
rect.hclust(clust3, k = 2)
```
*The first dendrogram uses euclidean distance with complete agglomeration, which merges clusters based on the maximum distance between points (furtherest neighbor). As we can observe, this method formed approximately three clusters with Yemen being a clear outlier (forming its own cluster) and two compact clusters forming. This is in stark contrast to Manhattan with Ward, which forms clusters by minimizing the total within cluster variance. In this dendrogram, the clusters are much more tightly grouped with less interal variance, with about 5 distinct cluster compared to the three in the previous method. Our third method uses maximum with single agglomeration. This method uses nearest neighbors, which forms clusters based on the smallest distance between points. This dendrogram is characterized by a singular outlier in Yemen that forms a distinct group along with every other country forming the other cluster.*

```{r}
source("https://raw.githubusercontent.com/jreuning/sds363_code/refs/heads/main/HClusEval3.R.txt")
#Call the function
hclus_eval(wbnorm, dist_m = 'euclidean', clus_m = 'complete', plot_op = T, print_num = 15)
```
*In determining the number of groups to retain, we can use different metrics. Looking at the total sum of squares (RSQ), the red line appears to have diminishing returns around the 4th cluster point. The semi-partial r-squared line in green has an elbow around the third index, which suggests a total of 3 groups. Lastly, the Root-mean-square standard deviation (RMSSTD) stops getting flat (from right to left) around the 4th-5th index. We can also look at the cluster distance chart for an elbow is around the 4th or potentially 6th index. Overall, these graphs indicate that the optimal number of groups to retain is four.*

```{r}
#kdata is just normalized input dataset
kdata <- wbnorm
n.lev <- 15  #set max value for number of clusters k

# Calculate the within groups sum of squared error (SSE) for the number of cluster solutions selected by the user
wss <- rnorm(10)
while (prod(wss==sort(wss,decreasing=T))==0) {
  wss <- (nrow(kdata)-1)*sum(apply(kdata,2,var))
  for (i in 2:n.lev) wss[i] <- sum(kmeans(kdata, centers=i)$withinss)}

# Calculate the within groups SSE for 250 randomized data sets (based on the original input data)
k.rand <- function(x){
  km.rand <- matrix(sample(x),dim(x)[1],dim(x)[2])
  rand.wss <- as.matrix(dim(x)[1]-1)*sum(apply(km.rand,2,var))
  for (i in 2:n.lev) rand.wss[i] <- sum(kmeans(km.rand, centers=i)$withinss)
  rand.wss <- as.matrix(rand.wss)
  return(rand.wss)
}

rand.mat <- matrix(0,n.lev,250)

k.1 <- function(x) { 
  for (i in 1:250) {
    r.mat <- as.matrix(suppressWarnings(k.rand(kdata)))
    rand.mat[,i] <- r.mat}
  return(rand.mat)
}

# Same function as above for data with < 3 column variables
k.2.rand <- function(x){
  rand.mat <- matrix(0,n.lev,250)
  km.rand <- matrix(sample(x),dim(x)[1],dim(x)[2])
  rand.wss <- as.matrix(dim(x)[1]-1)*sum(apply(km.rand,2,var))
  for (i in 2:n.lev) rand.wss[i] <- sum(kmeans(km.rand, centers=i)$withinss)
  rand.wss <- as.matrix(rand.wss)
  return(rand.wss)
}

k.2 <- function(x){
  for (i in 1:250) {
    r.1 <- k.2.rand(kdata)
    rand.mat[,i] <- r.1}
  return(rand.mat)
}

# Determine if the data data table has > or < 3 variables and call appropriate function above
if (dim(kdata)[2] == 2) { rand.mat <- k.2(kdata) } else { rand.mat <- k.1(kdata) }

# Plot within groups SSE against all tested cluster solutions for actual and randomized data - 1st: Log scale, 2nd: Normal scale

xrange <- range(1:n.lev)
yrange <- range(log(rand.mat),log(wss))
plot(xrange,yrange, type='n', xlab='Cluster Solution', ylab='Log of Within Group SSE', main='Cluster Solutions against Log of SSE')
for (i in 1:250) lines(log(rand.mat[,i]),type='l',col='red')
lines(log(wss), type="b", col='blue')
legend('topright',c('Actual Data', '250 Random Runs'), col=c('blue', 'red'), lty=1)

yrange <- range(rand.mat,wss)
plot(xrange,yrange, type='n', xlab="Cluster Solution", ylab="Within Groups SSE", main="Cluster Solutions against SSE")
for (i in 1:250) lines(rand.mat[,i],type='l',col='red')
lines(1:n.lev, wss, type="b", col='blue')
legend('topright',c('Actual Data', '250 Random Runs'), col=c('blue', 'red'), lty=1)

# Calculate the mean and standard deviation of difference between SSE of actual data and SSE of 250 randomized datasets
r.sse <- matrix(0,dim(rand.mat)[1],dim(rand.mat)[2])
wss.1 <- as.matrix(wss)
for (i in 1:dim(r.sse)[2]) {
  r.temp <- abs(rand.mat[,i]-wss.1[,1])
  r.sse[,i] <- r.temp}
r.sse.m <- apply(r.sse,1,mean)
r.sse.sd <- apply(r.sse,1,sd)
r.sse.plus <- r.sse.m + r.sse.sd
r.sse.min <- r.sse.m - r.sse.sd

# Plot differeince between actual SSE mean SSE from 250 randomized datasets - 1st: Log scale, 2nd: Normal scale 

xrange <- range(1:n.lev)
if (min(r.sse.min) < 0){
   yrange <- range(log(r.sse.plus - min(r.sse.min)*1.05), log(r.sse.min - min(r.sse.min)*1.05))
} else {
   yrange <- range(log(r.sse.plus), log(r.sse.min))
}

plot(xrange,yrange, type='n',xlab='Cluster Solution', ylab='Log of SSE - Random SSE', main='Cluster Solustions against (Log of SSE - Random SSE)')
lines(log(r.sse.m), type="b", col='blue')
lines(log(r.sse.plus), type='l', col='red')
lines(log(r.sse.min), type='l', col='red')
legend('topright',c('SSE - random SSE', 'SD of SSE-random SSE'), col=c('blue', 'red'), lty=1)

xrange <- range(1:n.lev)
yrange <- range(r.sse.plus,r.sse.min)
plot(xrange,yrange, type='n',xlab='Cluster Solution', ylab='SSE - Random SSE', main='Cluster Solutions against (SSE - Random SSE)')
lines(r.sse.m, type="b", col='blue')
lines(r.sse.plus, type='l', col='red')
lines(r.sse.min, type='l', col='red')
legend('topright',c('SSE - random SSE', 'SD of SSE-random SSE'), col = c('blue', 'red'), lty = 1)

# Ask for user input - Select the appropriate number of clusters
#choose.clust <- function(){readline("What clustering solution would you like to use? ")} 
#clust.level <- as.integer(choose.clust())
clust.level <- 4

# Apply K-means cluster solutions - append clusters to CSV file
fit <- kmeans(kdata, clust.level)
aggregate(kdata, by=list(fit$cluster), FUN=mean)
clust.out <- fit$cluster
kclust <- as.matrix(clust.out)
kclust.out <- cbind(kclust, wbnorm)
write.table(kclust.out, file="kmeans_out.csv", sep=",")

# Display Principal Components plot of data with clusters identified

clusplot(kdata, fit$cluster, shade = F, labels = 2, lines = 0, color = T, lty = 4, main = 'Principal Components plot showing K-means clusters')


#Make plot of five cluster solution in space desginated by first two
#  two discriminant functions

plotcluster(kdata, fit$cluster, main="Four Cluster Solution in DA Space",
            xlab="First Discriminant Function", ylab="Second Discriminant Function")

# end of script
```
*After running k-means clustering, we generated a sum of squares vs. k (number of clusters) plot. Looking at the actual data, the plot appears to have an elbow around the 4th index, which indicates that we should retain four groups. We also have a plot of the SSE vs number of clusters for random permutations of our data shown in red, which also appears to have a slight kink around the 4th index. We also have the log SSE against number of clusters where we are looking for the maximum, which again appears to be around the fourth index. All in all, k-means clustering suggests that the optimal number of groups to retain is 4. This follows our findings from hierarchical clustering where we also had the optimal number of groups being retained to be 4. These findings suggest that our data has a natural separated structure (into four different clusters). We also show the principle components of our K-mean clusters plotted against our first and second principle component in addition to our clusters in discriminant analysis space*


```{r}
clust1 <- hclust(dist1, method = "ward.D")
cuts <- cutree(clust1, k = 4)
clusplot(wbnorm, cuts, color = TRUE, shade = TRUE, labels = 2, lines = 0, main = "World Bank Five Cluster Plot, Complete Linkage, First two PC", cex = .5)
plotcluster(wbnorm, cuts, main = "Five Cluster Solution in DA Space", xlab = "First Discriminant Function", ylab = "Second Discriminant Function", cex = .8)
```

*The plots of our clusters shown in DA space and PCA space are shown above (using euclidean distance and Ward). There appears to be four clusters in total - the same as we found in k-means clustering. Both plots show clear separation between the clusters and supports the earlier selection of four groups to be the optimal number.*

```{r}
for (i in 1:4){
  print(paste("Countries in Cluster ", i))
  print(rownames(wbnorm)[cuts == i])
  print (" ")
}

pca <- prcomp(wbnorm, scale = TRUE)
pca$rotation  # This shows the loadings

```
*Cluster 1 (Uganda, Nigeria, Sudan, etc) appears to be composed of countries with low HDI indicators - they occupy the leftmost side of the first principle component, which appears to be a general indicator of all HDI statistics. In contrast, cluster 3 (Denmark, Finland, Japan, etc) appear to be the most developed countries with the strongest levels of HDI indicators with the highest score in relation to the first principle component. The second and third clusters are less clear but based on conjecture, the second cluster appears to be composed of countries that are currently in the development stage towards becoming developed countries (with countries like China in this cluster), occupying a higher first principle component than the other 2 clusters. Cluster 3 appears to be less developed and is more varied along the second principle component, which appears to measure things relating to gender development (as the variables male/female labor participation and gender dev index having large loadings). Overall, these clusters fit the context of our dataset. It groups countries based on their HDI standings in relation to various development statistics with highly developed countries occupying one cluster, low developed countries in another cluster, a cluster with countries in the middle of development, and lastly a cluster of countries that are transitioning to being highly developed.*



