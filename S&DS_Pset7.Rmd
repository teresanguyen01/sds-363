---
title: 'S&DS 363 Problem Set 7: Factor Analysis'
author: "Franklin Wu and Teresa Nguyen"
date: "2025-04-17"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Human Development Index Factor Analysis

Factor analysis is a statistical method used to examine a set of variables (indicators) and identify underlying relationships among them. The idea is that certain groups of indicators may be correlated because they are all influenced by some common, unobserved factors or constructs. By analyzing the correlation matrix of these indicators, we can uncover the presence of these latent factors and gain insight into their characteristics. In this homework, we will perform a factor analysis on the Human Development Index (HDI).

### Data Cleaning and Useful Packages

```{r}
library(psych)
library(car)
library(corrplot)
library(EFAtools)
```

```{r}

# From Problem Set 1
#This chunk defines several helpful functions

#n is the number of observations in the dataset
#p is the number of variables in the dataset

parallel<-function(n,p){
  
  if (n > 1000 || p > 100) {
    print ("Sorry, this only works for n<1000 and p<100")
    stop()
  }
  
  coefs <- matrix(
    c(0.0316, 0.7611, -0.0979, -0.3138, 0.9794, -.2059, .1226, 0, 0.1162, 
      0.8613, -0.1122, -0.9281, -0.3781, 0.0461, 0.0040, 1.0578, 0.1835, 
      0.9436, -0.1237, -1.4173, -0.3306, 0.0424, .0003, 1.0805 , 0.2578, 
      1.0636, -0.1388, -1.9976, -0.2795, 0.0364, -.0003, 1.0714, 0.3171, 
      1.1370, -0.1494, -2.4200, -0.2670, 0.0360, -.0024, 1.08994, 0.3809, 
      1.2213, -0.1619, -2.8644, -0.2632, 0.0368, -.0040, 1.1039, 0.4492, 
      1.3111, -0.1751, -3.3392, -0.2580, 0.0360, -.0039, 1.1173, 0.5309, 
      1.4265, -0.1925, -3.8950, -0.2544, 0.0373, -.0064, 1.1421, 0.5734, 
      1.4818, -0.1986, -4.2420, -0.2111, 0.0329, -.0079, 1.1229, 0.6460, 
      1.5802, -0.2134, -4.7384, -0.1964, 0.0310, -.0083, 1.1320),ncol=8, byrow=TRUE)
  
  calclim <- p
  if (p > 10) calclim <- 10
  coefsred <- coefs[1:calclim, ]
  temp <- c(p:1)
  #stick <- sort(cumsum(1/temp), decreasing=TRUE)[1:calclim]
  multipliers <- matrix(c(log(n),log(p),log(n)*log(p),1), nrow=1)
  longman <- exp(multipliers%*%t(coefs[,1:4]))[1:calclim]
  allen <- rep(NA, calclim)
  leig0 <- 0
  newlim <- calclim
  if (calclim+2 < p) newlim <-newlim+2
  for (i in 1:(newlim-2)){
    leig1 <- coefsred[i,5:8]%*%matrix(c(1,log(n-1),log((p-i-1)*(p-i+2)/2), leig0))
    leig0 <- leig1
    allen[i] <- exp(leig1)
  }
  pcompnum <- c(1:calclim)
  #data.frame(cbind(pcompnum,stick,longman,allen))
  data.frame(cbind(pcompnum,longman,allen))  
}

#########
#this function makes a nice plot if given the input from a PCA analysis
#created by prcomp()
##
#arguments are
#    n=number of observations

parallelplot <- function(comp){
  if (dim(comp$x)[1] > 1000 || length(comp$sdev) > 100) {
    print ("Sorry, this only works for n < 1000 and p < 100")
    stop()
  }
  #if (round(length(comp$sdev)) < round(sum(comp$sdev^2))) {
  #    print ("Sorry, this only works for analyses using the correlation matrix")
  #    stop()
  # }
  
  parallelanal <- parallel(dim(comp$x)[1], length(comp$sdev))
  print(parallelanal)
  calclim <- min(10, length(comp$sdev))
  eigenvalues <- (comp$sdev^2)[1:calclim]
  limits <- as.matrix(parallelanal[,2:3])
  limits <- limits[complete.cases(limits)]
  ymax <- range(c(eigenvalues),limits)
  plot(parallelanal$pcompnum, eigenvalues, xlab="Principal Component Number",
       ylim=c(ymax), ylab="Eigenvalues and Thresholds",
       main="Scree Plot with Parallel Analysis Limits",type="b",pch=15,lwd=2, col="red")
  #lines(parallelanal$pcompnum,parallelanal[,2], type="b",col="red",pch=16,lwd=2)
  lines(parallelanal$pcompnum,parallelanal[,2], type="b",col="green",pch=17,lwd=2)
  lines(parallelanal$pcompnum,parallelanal[,3], type="b",col="blue",pch=18,lwd=2)
  #legend((calclim/2),ymax[2],legend=c("Eigenvalues","Stick Method","Longman Method",
  # "Allen Method"),  pch=c(15:18), col=c("black","red","green","blue"),lwd=2)
  legend((calclim/2), ymax[2], legend=c("Eigenvalues","Longman Method","Allen Method"), 
         pch = c(16:18), col= c("red","green","blue"), lwd=2)
}


#make score plot with confidence ellipse.
#arguments are output from prcomp, vector with components for plotting 
# (usually c(1,2) or c(1,3)
#and a vector of names for the points

ciscoreplot<-function(x, comps, namevec){
  y1<-sqrt(5.99*(x$sdev[comps[1]]^2))
  ymod<-y1-y1%%.05
  y1vec<-c(-y1,seq(-ymod,ymod,by=0.05),y1)
  y2vecpos<-sqrt((5.99-(y1vec^2)/x$sdev[comps[1]]^2)*x$sdev[comps[2]]^2)
  y2vecneg<--sqrt((5.99-(y1vec^2)/x$sdev[comps[1]]^2)*x$sdev[comps[2]]^2)
  y2vecpos[1]<-0
  y2vecneg[1]<-0
  y2vecpos[length(y2vecpos)]<-0
  y2vecneg[length(y2vecneg)]<-0
  
  plot(x$x[,comps[1]],x$x[,comps[2]], 
       pch = 19, 
       cex = 1.2,
       xlim = c(min(y1vec, x$x[, comps[1]]), max(y1vec, x$x[, comps[1]])),
       ylim = c(min(y2vecneg, x$x[, comps[2]]), max(y2vecpos, x$x[, comps[2]])),
       main = "PC Score Plot with 95% CI Ellipse", 
       xlab = paste("Scores for PC", comps[1], sep = " "), 
       ylab = paste("Scores for PC", comps[2], sep = " "))
  
  lines(y1vec,y2vecpos,col="Red",lwd=2)
  lines(y1vec,y2vecneg,col="Red",lwd=2)
  outliers<-((x$x[,comps[1]]^2)/(x$sdev[comps[1]]^2)+(x$x[,comps[2]]^2)/(x$sdev[comps[2]]^2))>5.99
  
  points(x$x[outliers, comps[1]], x$x[outliers, comps[2]], pch = 19, cex = 1.2, col = "Blue")
  
  text(x$x[outliers, comps[1]],x$x[outliers, comps[2]], col = "Blue", lab = namevec[outliers])
}

```

Always need to start with this first. Read in Data, choose the columns that best fit our needs, and rename the columns.

```{r, include=FALSE}
data <- read.csv("../../Documents/sds-363/Human Development Index - Full.csv")
```

```{r}
# Based on going through the Kaggle dataset and choosing the most relevant features
data <- data[, c(
  "Country",
  "UNDP.Developing.Regions",
  "HDI.Rank..2021.",
  "Human.Development.Index..2021.",
  "Life.Expectancy.at.Birth..2021.",
  "Mean.Years.of.Schooling..2021.",
  "Gross.National.Income.Per.Capita..2021.",
  "Gender.Development.Index..2021.",
  "Coefficient.of.human.inequality..2021.",
  "Overall.loss......2021.",
  "Gender.Inequality.Index..2021.",
  "Maternal.Mortality.Ratio..deaths.per.100.000.live.births...2021.",
  "Adolescent.Birth.Rate..births.per.1.000.women.ages.15.19...2021.",
  "Labour.force.participation.rate..female....ages.15.and.older...2021.",
  "Labour.force.participation.rate..male....ages.15.and.older...2021.",
  "Carbon.dioxide.emissions.per.capita..production...tonnes...2021."
)]

# rename for better readability
colnames(data) <- c(
  "Country",
  "Region",
  "HDI_Rank_2021",
  "HDI_2021",
  "Life_Expectancy_2021",
  "Mean_Years_Schooling_2021",
  "GNI_Per_Capita_2021",
  "Gender_Dev_Index_2021",
  "Human_Inequality_Coeff_2021",
  "Overall_Loss_2021",
  "Gender_Inequality_Index_2021",
  "Maternal_Mortality_Rate_2021",
  "Adolescent_Birth_Rate_2021",
  "Female_Labour_Force_2021",
  "Male_Labour_Force_2021",
  "CO2_Emissions_percapita_2021"
)

data[c("GNI_Per_Capita_2021", "Gender_Dev_Index_2021", 
       "Maternal_Mortality_Rate_2021", "Adolescent_Birth_Rate_2021", 
       "CO2_Emissions_percapita_2021")] <- lapply(data[c("GNI_Per_Capita_2021", 
                                                         "Gender_Dev_Index_2021", 
                                                         "Maternal_Mortality_Rate_2021", 
                                                         "Adolescent_Birth_Rate_2021", 
                                                         "CO2_Emissions_percapita_2021" )], 
                                                  log)

rownames(data) <-data[,1]
data <- data[ , c("HDI_2021",
  "Life_Expectancy_2021",
  "Mean_Years_Schooling_2021",
  "GNI_Per_Capita_2021",
  "Gender_Dev_Index_2021",
  "Human_Inequality_Coeff_2021",
  "Overall_Loss_2021",
  "Gender_Inequality_Index_2021",
  "Maternal_Mortality_Rate_2021",
  "Adolescent_Birth_Rate_2021",
  "Female_Labour_Force_2021",
  "Male_Labour_Force_2021",
  "CO2_Emissions_percapita_2021"
)]

data <- na.omit(data)
```

### Correlation Matrix

The correlation matrix is a key diagnostic tool in factor analysis, used to assess whether a common or latent factor structure might exist among a set of variables. In this example, we examined the raw correlations and visualized them using a correlation plot:

```{r}
#Examine raw correlations
round(cor(data),2)

#Get correlation plot
corrplot.mixed(cor(data), lower.col = "black", upper = "ellipse", 
               tl.col = "black", number.cex = .5, tl.pos = "lt", 
               tl.cex = .7, p.mat = cor.mtest(data, conf.level = .95)$p, 
               sig.level = .05)


```

Factor analysis assumes that variables share common variance such as they cluster together based on high correlations, indicating the presence of underlying latent factors that explain the patterns in our data.

Based on our correlation and respective correlation plot, there appears to be strong correlations, both positive and negative, which suggests the presence of latent structures. For example, life expectancy is highly positively correlated with GNI per capita, mean years of schooling, and HDI, and strongly negatively correlated with maternal morality, adolescent birth rate, and overall loss, which supports the idea of a common development or health-related factor.

One exception is "female labor force participation", which has less statistically significant correlations and weaker correlations, in terms of magnitude, which may show that it does not align with the latent dimensions identified.  While life expectancy appears strongly connected to several development indicators, male and female labor force participation, along with the gender development index, show a more ambiguous relationship with the core structure of the data.

### KMO 

The Kaiser-Meyer-Olkin (KMO) statistic is a measure of sampling adequacy that evaluates the proportion of variance among variables that might be common variance. It indicates how suitable the data is for factor analysis by assessing the homogeneity of variables. Higher KMO values suggest that the variables share enough common variance to justify the use of factor analysis. 

```{r}
KMO(as.matrix(data))
```

Our data had an overall KMO of 0.858 and received a rating of "meritorious". This suggests that our variables have a substantial amount of shared variance that can be explained by latent factors, making our dataset suitable for factor analysis. Moreover, our individual KMO values for most of our variables are relatively charge, which suggests our selection of indicators is valid.

### Scree Plots and Parallel Analysis

```{r}
comp1 <- prcomp(data, scale. = T )

summary.PCA.JDRS <- function(x){
  sum_JDRS <- summary(x)$importance
  sum_JDRS[1, ] <- sum_JDRS[1, ]^2
  attr(sum_JDRS, "dimnames")[[1]][1] <- "Eigenvals (Variance)"
  sum_JDRS
}

round(summary.PCA.JDRS(comp1), 2)
#round(comp1$rotation, 2)

screeplot(comp1, type = "lines", col = "red", lwd = 2, pch = 19, cex = 1.2, 
          main = "Scree Plot of Transformed WB2013 Data")

parallelplot <- function(comp){
  if (dim(comp$x)[1] > 1000 || length(comp$sdev) > 100) {
    print ("Sorry, this only works for n < 1000 and p < 100")
    stop()
  }
  #if (round(length(comp$sdev)) < round(sum(comp$sdev^2))) {
  #    print ("Sorry, this only works for analyses using the correlation matrix")
  #    stop()
  # }
  
  parallelanal <- parallel(dim(comp$x)[1], length(comp$sdev))
  print(parallelanal)
  calclim <- min(10, length(comp$sdev))
  eigenvalues <- (comp$sdev^2)[1:calclim]
  limits <- as.matrix(parallelanal[,2:3])
  limits <- limits[complete.cases(limits)]
  ymax <- range(c(eigenvalues),limits)
  plot(parallelanal$pcompnum, eigenvalues, xlab="Principal Component Number",
       ylim=c(ymax), ylab="Eigenvalues and Thresholds",
       main="Scree Plot with Parallel Analysis Limits",type="b",pch=15,lwd=2, col="red")
  #lines(parallelanal$pcompnum,parallelanal[,2], type="b",col="red",pch=16,lwd=2)
  lines(parallelanal$pcompnum,parallelanal[,2], type="b",col="green",pch=17,lwd=2)
  lines(parallelanal$pcompnum,parallelanal[,3], type="b",col="blue",pch=18,lwd=2)
  #legend((calclim/2),ymax[2],legend=c("Eigenvalues","Stick Method","Longman Method",
  # "Allen Method"),  pch=c(15:18), col=c("black","red","green","blue"),lwd=2)
  legend((calclim/2), ymax[2], legend=c("Eigenvalues","Longman Method","Allen Method"), 
         pch = c(16:18), col= c("red","green","blue"), lwd=2)
}
parallelplot(comp1)

```
Based on the scree plot and parallel analysis, it suggests that we should use roughly 2 latent factors in our analysis - there is a slight elbow at 3 on the scree plot and the eigenvalue line on our parallel analysis chart is right above the cutoff. Our principal analysis shows that the first component explains roughly 71% of the variance while the second component adds about 12% of explained variance for a cumulative total of approximately 0.83.

### Series of Factor analyses

```{r}
fact1 <- factanal(data, factors = 2)
fact2 <- fa(data, nfactors = 2, fm = "pa")
fact3 <- fa(data, nfactors = 2, SMC = FALSE, fm = "pa")

repro1 <- fact1$loadings%*%t(fact1$loadings)
resid1 <- fact1$cor - repro1
len <- length(resid1[upper.tri(resid1)])
paste0(round(sum(rep(1, len)[abs(resid1[upper.tri(resid1)])>0.05])/len*100),"%")

repro2 <- fact2$loadings%*%t(fact2$loadings)
resid2 <- cor(data)-repro2
len <- length(resid2[upper.tri(resid2)])
paste0(round(sum(rep(1, len)[abs(resid2[upper.tri(resid2)])>0.05])/len*100),"%")

repro3 <- fact3$loadings%*%t(fact3$loadings)
resid3 <- cor(data)-repro3
len <- length(resid3[upper.tri(resid3)])
paste0(round(sum(rep(1, len)[abs(resid3[upper.tri(resid3)])>0.05])/len*100),"%")
```
Above, we applied three extraction methods: maximum likelihood, principal axis factoring, and iterative PCA (methods 1, 2, and 3, respectively). We then evaluated the accuracy of each method by calculating the number of residual correlations greater than a cutoff value of 0.05. This involved computing the residual correlation matrix (the difference between the observed correlations and the reproduced ones) and determining the percentage of residuals exceeding the threshold. The results were 24% for maximum likelihood, 31% for principal axis factoring, and 29% for iterative PCA. These findings suggest that maximum likelihood is the best extraction method for our data, as it leaves the least unexplained correlation after modeling based on its factors.

### Extraction Methods, Varimax, and Quartimax

```{r}
(fact_final <- factanal(data, factors = 2, rotation = "varimax"))
fa_result <- fa(data, nfactors = 2, rotate = "quartimax", fm = "ml")
print(fa_result$loadings, digits = 3, cutoff = 0.3)

plot(fact_final$loadings, pch = 18, col = 'red')
abline(h = 0)
abline(v = 0)
text(fact_final$loadings, labels = names(data), cex = 0.8)

plot(fa_result$loadings, pch = 18, col = 'blue')
abline(h = 0)
abline(v = 0)
text(fa_result$loadings, labels = names(data), cex = 0.8)
```

We identified maximum likelihood to be the most robust extraction method and we estimated the factor loadings by maximizing the likelihood of observing our sample correlation matrix. We tested both varimax and quartimax rotations. 

Varimax rotations work by making each factor load strongly on a set of different variables, helping to distinguish our factors. Quartimax rotations makes each variable load strongly on one factor, ignoring if factors are blended. Using maximum likelihood with a varimax rotation for 2 factors, the first factor has significant loadings on human inequality coefficient, overall loss, gender inequality, which are all metrics relating to measurment of equality in a country. In contrast, the second factor has relatively high loadings in HDI, GNI per capita, and CO2 emissions, which hint at being more general indicators for development. A quartimax rotation gave us a vastly different result with high loadings on nearly every variable for the first factor apart from the female and male labor force participation while the second factor was just composed of human inequality and overall loss. This suggests under a quartimax rotation, the first factor comprises most of the variability (using all the indicators) while the second factor is the less over variance that was not captured.

Looking at our loading plots, the plot using the varimax rotation indicates 3 distinct clusters. The one in the top right-negative in factor 1 and positive in factor 2-captures wealth/development in relation to inequality, while the cluster in the bottom right-with positive loading in factor 1 and weak loadings in factor 2-likely point to a measure of inequality. The 2 variables in the middle (female and male labor force) have weak loadings in both, suggesting they do not contribute significantly to either factor. The rough U shape from our plot using a quartimax rotation corroborates our findings, as the variables on the left are associated with negative factors of development and the variables on the right positive factors of development, with the labor force participation variables having weak loadings overall.

Overall, our data was suitable for factor analysis. We identified two significant factors, using parallel analysis and a scree plot. After testing three different extraction methods, maximum likelihood, principal axis factoring, and iterative PCA, and based on the residual correlation matrix, we identified maximum likeihood to be the best. Running both varimax and quartimax rotations and looking at the loading plots helped provide interpretation for our factors. Overall, using our analysis from the varimax rotation, factor 1 appears to be a measure of overall inequality in a country (positive meaning more inequality and vice versa) while factor 2 is a measurment of overall development.
