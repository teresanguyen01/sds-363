---
title: 'Final Project'
author: "Franklin Wu and Teresa Nguyen"
date: "2025-04-09"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE, echo = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Introduction**

<!-- ![World Map of Countries by HDI]("../../Documents/sds-363/HDI2022Incrimental.svg.png") -->

Our project utilizes the 2021 Human Development Index (HDI) dataset sourced from Kaggle. This dataset includes key measures of each country's achievements in health, education, and standard of living which are critical dimensions of overall development. HDI is widely used by the United Nations, policymakers, and academics to assess development progress, prioritize aid, tailor policy responses, and explore the connections between development and societal outcomes. In addition, multinational corporations often reference HDI metrics when shaping market entry strategies, with higher scores suggesting more stable and opportunistic markets.

We selected this dataset for multivariate analysis for several key reasons. First, it offers global coverage across nearly all countries and regions, and it is timely, reflecting data from the past four years. Additionally, HDI captures multiple aspects of development, allowing us to examine the relationships between economic, social, and health factors simultaneously. This makes it well-suited for multivariate analysis, as we aim to uncover underlying patterns and drivers of human development. Lastly, the project's findings have meaningful policy implications, which are particularly interesting to us given our respective backgrounds in healthcare engineering and economics, and our shared interest for data analysis, policy, and real-world events.

**Design and Primary Questions**

Our analysis utilizes three primary multivariate methods, supported by supplementary techniques: Principal Component Analysis (PCA), Cluster Analysis, and Discriminant Analysis.

For PCA, we seek to answer: *"What are the principal dimensions that explain the variation in HDI across countries?"* We will assess the assumptions of PCA, perform the analysis, determine the number of components to retain using a scree plot and parallel analysis, and interpret and visualize the final retained components.

Using Cluster Analysis, we aim to answer: *"Based on key developmental metrics, are there natural groupings of countries?"* We will test different distance metrics and agglomeration methods, visualize the clustering structure with dendrograms, determine the optimal number of clusters, interpret the characteristics of each cluster, and supplement the analysis with k-means clustering.

Lastly, through Discriminant analysis, we explore: *"How accurately can we predict a country's geographic region based on a subset of developmental characteristics?"* We will assess multivariate normality, perform Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), and Stepwise Discriminant Analysis, evaluate classification accuracy, and derive and interpret the discriminant functions in the context of our HDI dataset.

**Description of Variables**

| **Variable**                   | **Type**       | **Description**                                                                                  |
|----------------------------------|----------------|--------------------------------------------------------------------------------------------------|
| Country                          | Categorical    | Name of the country.                                                                             |
| Region                           | Categorical    | Geographic region classification (e.g., Sub-Saharan Africa, Europe and Central Asia, etc.).      |
| Human Development Groups         | Categorical    | Classification of country based on HDI status (Very High, High, Medium, Low).                    |
| HDI_Rank_2021                    | Continuous     | Ranking of countries based on their HDI in 2021.                                                 |
| HDI_2021                         | Continuous     | HDI value in 2021, a composite measure based on life expectancy, education, and GNI per capita.  |
| Life_Expectancy_2021             | Continuous     | Average number of years a newborn is expected to live.                                           |
| Mean_Years_Schooling_2021        | Continuous     | Average number of completed years of education among people aged 25 years and older.             |
| GNI_Per_Capita_2021              | Continuous     | Gross National Income per capita.                                                               |
| Gender_Dev_Index_2021            | Continuous     | Measurement of gender gap based on holistic metrics.                                             |
| Human_Inequality_Coeff_2021      | Continuous     | Percentage loss in HDI due to inequality.                                                        |
| Overall_Loss_2021                | Continuous     | Overall loss percentage in human development outcomes due to inequality.                        |
| Gender_Inequality_Index_2021     | Continuous     | Measurement of gender disparities across health, empowerment, and labor market participation.   |
| Maternal_Mortality_Rate_2021     | Continuous     | Number of maternal deaths per 100,000 live births.                                               |
| Adolescent_Birth_Rate_2021       | Continuous     | Number of births per 1,000 women aged 15–19.                                                     |
| Female_Labour_Force_2021         | Continuous     | Percentage of the female population participating in the labor force.                           |
| Male_Labour_Force_2021           | Continuous     | Percentage of the male population participating in the labor force.                             |
| CO2_Emissions_percapita_2021     | Continuous     | Carbon dioxide emissions per capita (metric tons).                                               |


## Data Initialization

```{r, echo = FALSE}
data <- read.csv("../../Documents/sds-363/Human Development Index - Full.csv")

data <- data[, c("Country", "UNDP.Developing.Regions", "HDI.Rank..2021.", "Human.Development.Index..2021.", "Life.Expectancy.at.Birth..2021.", "Mean.Years.of.Schooling..2021.", "Gross.National.Income.Per.Capita..2021.", "Gender.Development.Index..2021.", "Coefficient.of.human.inequality..2021.", "Overall.loss......2021.", "Gender.Inequality.Index..2021.", "Maternal.Mortality.Ratio..deaths.per.100.000.live.births...2021.", "Adolescent.Birth.Rate..births.per.1.000.women.ages.15.19...2021.", "Labour.force.participation.rate..female....ages.15.and.older...2021.", "Labour.force.participation.rate..male....ages.15.and.older...2021.", "Carbon.dioxide.emissions.per.capita..production...tonnes...2021.")]

# rename for better readability
colnames(data) <- c("Country", "Region", "HDI_Rank_2021", "HDI_2021", "Life_Expectancy_2021", "Mean_Years_Schooling_2021", "GNI_Per_Capita_2021", "Gender_Dev_Index_2021", "Human_Inequality_Coeff_2021", "Overall_Loss_2021", "Gender_Inequality_Index_2021", "Maternal_Mortality_Rate_2021", "Adolescent_Birth_Rate_2021", "Female_Labour_Force_2021", "Male_Labour_Force_2021", "CO2_Emissions_percapita_2021")

# our data frame should only include complete cases
data <- data[complete.cases(data), ]
```


**Plots and Transformations**

We began our initial analysis of the data using scatterplots and heatmaps to assess linearity, correlations, and the overall suitability of the data for multivariate methods. Additionally, we checked for the presence of outliers and non-linear patterns.

```{r, warning=FALSE, echo = FALSE, message=FALSE}
library(corrplot)
library(PerformanceAnalytics)

corrplot.mixed(
  cor(data[, -c(1:3)]),
  lower.col = "black",
  upper = "ellipse",
  tl.col = "black",
  number.cex = 0.3,
  order = "hclust",
  tl.pos = "lt",
  tl.cex = 0.5
)


chart.Correlation(data[, -c(1:3)])
```

Based on the correlation plots, there are strong positive correlations between variables such as HDI, Life Expectancy, Mean Years of Schooling, and GNI per Capita, all above 0.75. Likewise, variables such as the Gender Inequality Index and Human Inequality Coefficient are strongly negatively correlated with HDI and schooling variables. These strong relationships suggest a shared structure in the data, making PCA appropriate.

In examining scatterplots, most relationships appear linear; however, certain relationships, such as between GNI per Capita and Maternal Mortality Rate, show signs of non-linearity. This suggests that transformations (ex: a log transformation) could be beneficial.

Regarding distributions, variables such as HDI, Life Expectancy, and Labour Force Participation appear approximately normally distributed, while variables like GNI per Capita and CO₂ Emissions per Capita are heavily right-skewed. We may consider applying transformations to these skewed variables before proceeding.

### Multivariate Normality Check and Transformations

```{r, echo = FALSE, message=FALSE, warning=FALSE}
library(MASS)
library(car)   
data1 <- as.matrix(data[, -c(1:3)])  

center <- colMeans(data1)  # Mean vector
cov_matrix <- cov(data1)   # Covariance matrix
mahalanobis_dist <- mahalanobis(data1, center, cov_matrix)

theoretical_quantiles <- qchisq(ppoints(length(mahalanobis_dist)), df = ncol(data1))

qqPlot(mahalanobis_dist, distribution = "chisq", df = ncol(data1),
       main = "Chi-Square Q-Q Plot for Multivariate Normality",
       xlab = "Theoretical Quantiles",
       ylab = "Observed Mahalanobis Distances")

# MAKE chi-square quantile plot
#library(heplots)
#cqplot(data[, -c(1:3)], main = "World Bank Data")
```

Based on the Chi-square Q-Q plot of Mahalanobis distances, several observations deviate substantially from the expected linear trend, particularly in the upper quantiles. This upward deviation (heavy-tailed or right-skewed pattern) suggests that the data may not satisfy the assumption of multivariate normality. This is especially important for methods such as Linear Discriminant Analysis (LDA), which assumes multivariate normality within each group. To address this issue, we applied log transformations to selected skewed variables (as shown below) in an effort to improve the distributional properties and better meet the assumptions of our multivariate methods.

```{r, warning=FALSE, echo = FALSE}
# Apply log transformation to specific columns 
data[c(
  "GNI_Per_Capita_2021", 
  "Gender_Dev_Index_2021", 
  "Maternal_Mortality_Rate_2021", 
  "Adolescent_Birth_Rate_2021", 
  "CO2_Emissions_percapita_2021"
)] <- lapply(
  data[c(
    "GNI_Per_Capita_2021", 
    "Gender_Dev_Index_2021", 
    "Maternal_Mortality_Rate_2021", 
    "Adolescent_Birth_Rate_2021", 
    "CO2_Emissions_percapita_2021"
  )], 
  log
)

chart.Correlation(data[, -c(1:3)])
```

We applied log transformations to the following variables: GNI per Capita, Gender Development Index, Maternal Mortality Rate, Adolescent Birth Rate, and CO₂ Emissions per Capita. These variables were initially right-skewed and showed non-linear relationships with other variables, which could impact the validity of multivariate techniques that assume linearity and normality.

After transformation, the distributions of these variables appear more symmetric and approximately normal, as seen in the updated scatterplot matrix. Relationships between variables also appear more linear, with tighter and more consistent correlation patterns. These improvements enhance the interpretability of our multivariate methods, particularly PCA, clustering, and discriminant analysis, by aligning better with their underlying statistical assumptions.

***Principal Component Analysis (PCA)***

```{r, echo = FALSE}
data1 <- as.matrix(data[, -c(1:3)])  

center <- colMeans(data1)  # Mean vector
cov_matrix <- cov(data1)   # Covariance matrix
mahalanobis_dist <- mahalanobis(data1, center, cov_matrix)

theoretical_quantiles <- qchisq(ppoints(length(mahalanobis_dist)), df = ncol(data1))

qqPlot(mahalanobis_dist, distribution = "chisq", df = ncol(data1),
       main = "Chi-Square Q-Q Plot for Multivariate Normality",
       xlab = "Theoretical Quantiles",
       ylab = "Observed Mahalanobis Distances")
```

After applying log transformations to variables with non-normal distributions, the updated Chi-square Q-Q plot shows an improvement. Most points now lie closer to the theoretical quantile line and fall within the confidence bounds, indicating better alignment with multivariate normality. While a few observations (notably cases 176 and 192) still appear as multivariate outliers and curve upward, the overall distribution more closely resembles that of a multivariate normal distribution.

This is crucial for Principal Component Analysis (PCA), which assumes linear relationships and benefits from underlying multivariate normality to meaningfully reduce dimensionality. The Mahalanobis distances now largely conform to the chi-square distribution, supporting the appropriateness of using PCA on the transformed data.

### Correlation Analysis and PCA Suitability

For our next step, we created correlation plots to determine correlation coefficients and correlations. We order it based on first principal component to identify the correlated groups and analyze whether PCA is a suitable method. 

```{r, echo = FALSE}
corrplot(cor(data[ ,-c(1:3)]), method="number", order="FPC", tl.cex = .5, 
         number.cex = .3)
corrplot(cor(data[ ,-c(1:3)]),method = "ellipse", order="FPC", tl.cex = .5)

dim(data)
```

To assess whether Principal Component Analysis (PCA) is appropriate for our dataset, we examined the correlation structure among the continuous variables. The correlation plots, ordered by the first principal component, showed many strong relationships, both positive and negative, indicating substantial shared variance across variables which is a key condition for effective PCA.

Several variables displayed strong positive correlations. For example, the Overall Loss and Gender Inequality Index had a correlation of 0.88, as did Adolescent Birth Rate and Human Inequality Coefficient. The Gender Inequality Index was also highly correlated with Adolescent Birth Rate (r = 0.81). 

In contrast, we observed strong negative correlations between Mean Years of Schooling and the Human Inequality Coefficient (r = -0.89), between Gender Inequality and Life Expectancy (r = -0.85), and between Human Inequality and Life Expectancy (r = -0.85). These relationships suggest that many variables are capturing overlapping information, which PCA can help summarize efficiently by reducing the dataset to a smaller set of uncorrelated components.

Our dataset consists of 150 complete observations and 13 continuous variables (after excluding three categorical ones), resulting in a sample size more than ten times larger than the number of variables. This satisfies a common rule of thumb for PCA suitability. Given the strong correlations and adequate sample size, we conclude that PCA is an appropriate method for exploring and simplifying the structure of this dataset.


```{r, echo = FALSE, message=FALSE, results='hide'}

summary.PCA.JDRS <- function(x){
  sum_JDRS <- summary(x)$importance
  sum_JDRS[1, ] <- sum_JDRS[1, ]^2
  attr(sum_JDRS, "dimnames")[[1]][1] <- "Eigenvals (Variance)"
  sum_JDRS
}

pc1 <- prcomp(data[, -c(1:3)], scale. = TRUE)

round(summary.PCA.JDRS(pc1),2)

```

|                   | PC1  | PC2  | PC3  | PC4  | PC5  | PC6  | PC7  | PC8  | PC9  | PC10 | PC11 | PC12 | PC13 |
|-------------------|------|------|------|------|------|------|------|------|------|------|------|------|------|
| Eigenvals (Variance)   | 9.21 | 1.60 | 0.74 | 0.46 | 0.33 | 0.23 | 0.13 | 0.11 | 0.10 | 0.06 | 0.03 | 0.01 | 0    |
| Proportion of Variance | 0.71 | 0.12 | 0.06 | 0.04 | 0.03 | 0.02 | 0.01 | 0.01 | 0.01 | 0.00 | 0.00 | 0.00 | 0    |
| Cumulative Proportion  | 0.71 | 0.83 | 0.89 | 0.92 | 0.95 | 0.97 | 0.98 | 0.98 | 0.99 | 1.00 | 1.00 | 1.00 | 1.00 |


Based on the summary of principal components, the first component explains approximately 71% of the total variance, while the second explains an additional 12%, bringing the cumulative explained variance to 83% with just two components. Using the common threshold of 80% total variance explained as a criterion for component retention, we find that the first two principal components are sufficient to capture the majority of the dataset’s variability.

Additionally, applying the eigenvalue-greater-than-1 rule (Kaiser criterion), we would again retain only the first two components, which have eigenvalues of 9.21 and 1.60, respectively. All remaining components have eigenvalues below 1 and contribute minimally to the overall variance. Overall, these results suggest that retaining two principal components is sufficient for our analysis, allowing us to reduce dimensionality while still capturing the majority of the variance present in the dataset.

### Scree plots

We used a scree plot to visualize the eigenvalues associated with each principal component. The plot shows a clear "elbow" at the second principal component, where the steep drop in variance levels off. This suggests that the majority of the variance is explained by the first two components, with a sharp decline afterward.

```{r, echo = FALSE}
screeplot(pc1, type = "lines", col = "red", lwd = 2, pch = 19, cex = 1.2, 
          main = "Scree Plot of Raw WB Data")
```

According to the elbow method, we typically retain components up to the point where the curve starts to flatten, which in this case includes the first and second principal components. While there is a very slight slope change near PC3, it is not substantial enough to justify its inclusion. Therefore, the scree plot supports retaining the first two components for further analysis, consistent with both the cumulative variance explained and the eigenvalue-greater-than-one rule.


```{r, echo = FALSE}
parallel<-function(n,p){
  
  if (n > 1000 || p > 100) {
    print ("Sorry, this only works for n<1000 and p<100")
    stop()
  }
  
  coefs <- matrix(
    c(0.0316, 0.7611, -0.0979, -0.3138, 0.9794, -.2059, .1226, 0, 0.1162, 
      0.8613, -0.1122, -0.9281, -0.3781, 0.0461, 0.0040, 1.0578, 0.1835, 
      0.9436, -0.1237, -1.4173, -0.3306, 0.0424, .0003, 1.0805 , 0.2578, 
      1.0636, -0.1388, -1.9976, -0.2795, 0.0364, -.0003, 1.0714, 0.3171, 
      1.1370, -0.1494, -2.4200, -0.2670, 0.0360, -.0024, 1.08994, 0.3809, 
      1.2213, -0.1619, -2.8644, -0.2632, 0.0368, -.0040, 1.1039, 0.4492, 
      1.3111, -0.1751, -3.3392, -0.2580, 0.0360, -.0039, 1.1173, 0.5309, 
      1.4265, -0.1925, -3.8950, -0.2544, 0.0373, -.0064, 1.1421, 0.5734, 
      1.4818, -0.1986, -4.2420, -0.2111, 0.0329, -.0079, 1.1229, 0.6460, 
      1.5802, -0.2134, -4.7384, -0.1964, 0.0310, -.0083, 1.1320),ncol=8, byrow=TRUE)
  
  calclim <- p
  if (p > 10) calclim <- 10
  coefsred <- coefs[1:calclim, ]
  temp <- c(p:1)
  #stick <- sort(cumsum(1/temp), decreasing=TRUE)[1:calclim]
  multipliers <- matrix(c(log(n),log(p),log(n)*log(p),1), nrow=1)
  longman <- exp(multipliers%*%t(coefs[,1:4]))[1:calclim]
  allen <- rep(NA, calclim)
  leig0 <- 0
  newlim <- calclim
  if (calclim+2 < p) newlim <-newlim+2
  for (i in 1:(newlim-2)){
    leig1 <- coefsred[i,5:8]%*%matrix(c(1,log(n-1),log((p-i-1)*(p-i+2)/2), leig0))
    leig0 <- leig1
    allen[i] <- exp(leig1)
  }
  pcompnum <- c(1:calclim)
  #data.frame(cbind(pcompnum,stick,longman,allen))
  data.frame(cbind(pcompnum,longman,allen))  
}

#########
#this function makes a nice plot if given the input from a PCA analysis
#created by prcomp()
##
#arguments are
#    n=number of observations

parallelplot <- function(comp){
  if (dim(comp$x)[1] > 1000 || length(comp$sdev) > 100) {
    print ("Sorry, this only works for n < 1000 and p < 100")
    stop()
  }
  #if (round(length(comp$sdev)) < round(sum(comp$sdev^2))) {
  #    print ("Sorry, this only works for analyses using the correlation matrix")
  #    stop()
  # }
  
  parallelanal <- parallel(dim(comp$x)[1], length(comp$sdev))
  print(parallelanal)
  calclim <- min(10, length(comp$sdev))
  eigenvalues <- (comp$sdev^2)[1:calclim]
  limits <- as.matrix(parallelanal[,2:3])
  limits <- limits[complete.cases(limits)]
  ymax <- range(c(eigenvalues),limits)
  plot(parallelanal$pcompnum, eigenvalues, xlab="Principal Component Number",
       ylim=c(ymax), ylab="Eigenvalues and Thresholds",
       main="Scree Plot with Parallel Analysis Limits",type="b",pch=15,lwd=2, col="red")
  #lines(parallelanal$pcompnum,parallelanal[,2], type="b",col="red",pch=16,lwd=2)
  lines(parallelanal$pcompnum,parallelanal[,2], type="b",col="green",pch=17,lwd=2)
  lines(parallelanal$pcompnum,parallelanal[,3], type="b",col="blue",pch=18,lwd=2)
  #legend((calclim/2),ymax[2],legend=c("Eigenvalues","Stick Method","Longman Method",
  # "Allen Method"),  pch=c(15:18), col=c("black","red","green","blue"),lwd=2)
  legend((calclim/2), ymax[2], legend=c("Eigenvalues","Longman Method","Allen Method"), 
         pch = c(16:18), col= c("red","green","blue"), lwd=2)
}

parallelplot(pc1)
```

Since parallel analysis assumes that the data is suitable for PCA, it is a valid method to determine the number of principal components to retain. After applying log transformations, our data shows approximately linear relationships among variables and improved normality, although strict normality is not required for PCA.

Based on the results of parallel analysis, we should retain two principal components, as the observed eigenvalues for the first two components exceed the corresponding random thresholds (Longman and Allen methods). After the second component, the observed eigenvalues fall below the simulated thresholds, indicating that additional components do not explain more variance than would be expected by chance. Therefore, the parallel analysis supports retaining two principal components for further analysis.

### Interpretation of PCA

```{r, echo = FALSE}
# principal component loadings to indicate how much of each original var.
# contributes to each principal component
round(pc1$rotation,2)
```

Based on our analysis, we decided to retain the first two principal components. The first principal component appears to represent a general measure of overall well-being and development. It is characterized by strong positive loadings on variables such as HDI, Life Expectancy, and Mean Years of Schooling, which are key benchmarks for assessing a country's development. Conversely, this component shows large negative loadings on variables related to inequality, including the Human Inequality Coefficient, Overall Loss, Gender Inequality Index, and Maternal Mortality Rate. These negative loadings reflect dimensions associated with lower levels of development, suggesting that PC1 captures a spectrum ranging from highly developed to less developed countries.

The second principal component appears to capture aspects of gender equality. It is characterized by large loadings for the Gender Development Index, Female Labour Force Participation, and Male Labour Force Participation. Together, these variables reflect patterns related to gender-based disparities in economic participation and development. This component helps distinguish countries not just by their overall level of development but specifically by their gender equality dynamics, providing an important complementary dimension to the first principal component.

### Visualizations and Validations of PCA Results

We want to see how observations of countries are distributed among the two principal components and identify outliers, which deviate from multivariate normal distributions of scores.

We also create a biplot to help understand how the observations and variables interact in a reduced dimensional space. 

```{r, echo = FALSE}

ciscoreplot<-function(x, comps, namevec){
  y1<-sqrt(5.99*(x$sdev[comps[1]]^2))
  ymod<-y1-y1%%.05
  y1vec<-c(-y1,seq(-ymod,ymod,by=0.05),y1)
  y2vecpos<-sqrt((5.99-(y1vec^2)/x$sdev[comps[1]]^2)*x$sdev[comps[2]]^2)
  y2vecneg<--sqrt((5.99-(y1vec^2)/x$sdev[comps[1]]^2)*x$sdev[comps[2]]^2)
  y2vecpos[1]<-0
  y2vecneg[1]<-0
  y2vecpos[length(y2vecpos)]<-0
  y2vecneg[length(y2vecneg)]<-0
  
  plot(x$x[,comps[1]],x$x[,comps[2]], 
       pch = 19, 
       cex = 1.2,
       xlim = c(min(y1vec, x$x[, comps[1]]), max(y1vec, x$x[, comps[1]])),
       ylim = c(min(y2vecneg, x$x[, comps[2]]), max(y2vecpos, x$x[, comps[2]])),
       main = "PC Score Plot with 95% CI Ellipse", 
       xlab = paste("Scores for PC", comps[1], sep = " "), 
       ylab = paste("Scores for PC", comps[2], sep = " "))
  
  lines(y1vec,y2vecpos,col="Red",lwd=2)
  lines(y1vec,y2vecneg,col="Red",lwd=2)
  outliers<-((x$x[,comps[1]]^2)/(x$sdev[comps[1]]^2)+(x$x[,comps[2]]^2)/(x$sdev[comps[2]]^2))>5.99
  
  points(x$x[outliers, comps[1]], x$x[outliers, comps[2]], pch = 19, cex = 1.2, col = "Blue")
  
  text(x$x[outliers, comps[1]],x$x[outliers, comps[2]], col = "Blue", lab = namevec[outliers])
}

ciscoreplot(pc1, c(1, 2), data[, 1])

```

The score plot visualizes the distribution of countries based on the first and second principal components, with a 95% confidence ellipse overlaid to help identify potential outliers and assess the overall spread of observations. There do not appear to be any distinct clusters or groupings within the plot based on the two retained components.

Two countries, Yemen and Madagascar, fall outside the 95% confidence ellipse. Madagascar deviates slightly, primarily along the first principal component axis, suggesting modest differences in the factors captured by PC1. In contrast, Yemen lies well outside the ellipse along the second principal component axis, indicating a more substantial deviation related to the factors captured by PC2, possibly linked to gender equality dynamics.

Our interpretation of the 95% confidence ellipse is appropriate, as the assumption of approximate multivariate normality, verified earlier using the Chi-square Q-Q plot, is necessary for the valid use of confidence ellipses in multivariate space.

```{r, echo = FALSE}
biplot(pc1, choices = c(1, 2), pc.biplot = TRUE, cex = 0.7)
```

The biplot shows the distribution of countries based on their principal component scores for the first and second principal components. The arrows indicate the strength (magnitude) of each variable's contribution, while their direction illustrates how the variables contribute to the principal components. Similar to the earlier score plot, there are no clear distinct clusters or trends among the countries based on their principal component scores. However, the directions of the arrows reinforce our earlier interpretation of the principal components.

The second principal component is primarily influenced by variables such as female and male labor force participation and the Gender Development Index, whose arrows point upward along PC2. In contrast, the first principal component is influenced by development-related variables, such as GNI per capita and mortality rate, which point in opposite directions, reflecting their negative correlation. When two arrows are approximately perpendicular, it indicates that the corresponding variables have smaller loadings for that particular principal component, as observed in the biplot.

Based on our findings, PCA was an appropriate method for this dataset. The variables showed strong correlations, linear relationships, and approximate multivariate normality (though normality is not strictly required for PCA, it allowed us to apply methods such as parallel analysis more reliably). We successfully captured approximately 83% of the total variance using two principal components, as determined through the scree plot, parallel analysis, and the eigenvalue-greater-than-one criterion.

The first principal component primarily captures overall development, with development-related variables loading strongly positively, and indicators of lower development, such as inequality and mortality, loading strongly negatively. The second principal component captures aspects of gender equality, driven by variables related to female and male labor force participation and the gender inequality indices.

Finally, we created a score plot with a 95% confidence ellipse and the biplot to further validate our findings. The score plot revealed two outliers, Yemen and Madagascar, which is expected given the sample size of over 150 countries. The deviation for Madagascar was minor along the second component, while Yemen showed a more pronounced deviation. The biplot further confirmed the interpretations of the principal components and was appropriate to use given that the assumption of multivariate normality was approximately satisfied.

Overall, PCA successfully reduced the dataset to two interpretable components, capturing major patterns in development and gender equality for further analysis.

***Cluster Analysis***

```{r, echo = FALSE, warning=FALSE}
library(aplpack)
library(fpc)
library(cluster)
library(ape)
library(amap)
library(PerformanceAnalytics)
```

### Distance Metrics & Possible Transformations

```{r, echo = FALSE}
#one way to standardize data
wbnorm <- data[, c("HDI_2021", "Life_Expectancy_2021", 
                   "Mean_Years_Schooling_2021", "GNI_Per_Capita_2021", 
                   "Gender_Dev_Index_2021", "Gender_Inequality_Index_2021", 
                   "Maternal_Mortality_Rate_2021", "Adolescent_Birth_Rate_2021", 
                   "Female_Labour_Force_2021", "Male_Labour_Force_2021", 
                   "CO2_Emissions_percapita_2021")]
rownames(wbnorm) <- data[, 1]
wbnorm <- scale(na.omit(wbnorm))
print("dimensions of the standardized data set")
dim(wbnorm)

```

We chose Euclidean distance as the most appropriate metric for our cluster analysis because our dataset consists of continuous, standardized variables. Euclidean distance is ideal for measuring straight-line similarity in multivariate numeric data, and standardization ensures that all variables contribute equally to the distance calculations.

Other distance metrics were considered but found unsuitable for this context. For example, the Jaccard distance is designed for binary or presence-absence data, which does not apply here, and Manhattan distance, which measures absolute differences, is less effective when variables are on different scales or when squared differences (as in Euclidean) better capture dissimilarity.

To support the use of Euclidean distance, we applied preprocessing steps such as standardizing all variables, log-transforming skewed indicators, and assigning country names as row labels for interpretability. These steps support the reliability of clustering results by ensuring comparability and minimizing the influence of outliers or variable scale.

### Hierarchical Cluster Analysis - Euclidean and Manhattan

```{r, echo = FALSE}
#Euclidean and complete
dist1 <- dist(wbnorm, method = "euclidean")
clust1 <- hclust(dist1)
plot(clust1, labels = rownames(wbnorm), cex = 0.3, xlab = "", 
     ylab = "Distance", main = "Clustering of Countries")
rect.hclust(clust1, k = 3)

#Ward with Manhattan
dist2 <- dist(wbnorm, method = "manhattan")
clust2 <- hclust(dist2, method = "ward.D")
plot(clust2, labels = rownames(wbnorm), cex = 0.3, xlab = "", 
     ylab = "Distance", main = "Clustering of Countries")
rect.hclust(clust2, k = 5)

#Maximum with Single
dist3 <- dist(wbnorm, method = "maximum")
clust3 <- hclust(dist3, method = "single")
plot(clust3, labels = rownames(wbnorm), cex = 0.3, xlab = "", 
     ylab = "Distance", main = "Clustering of Countries")
rect.hclust(clust3, k = 2)
```

The first dendrogram uses Euclidean distance with complete linkage (also known as the farthest-neighbor method), which merges clusters based on the maximum distance between any pair of points. This approach produced approximately three main clusters. Notably, Yemen emerged as a clear outlier, forming its own distinct cluster, while the remaining countries were divided into two relatively compact groups.

In contrast, the second dendrogram applies Manhattan distance with Ward’s method, which seeks to minimize the total within-cluster variance at each step. This resulted in five well-defined clusters with more homogeneous groupings. Compared to complete linkage, Ward’s method produced clusters with greater internal consistency and smaller within-group differences.

The third dendrogram uses maximum distance with single linkage, which merges clusters based on the smallest distance between any two points (nearest-neighbor). This method yielded a much less structured result: Yemen again appeared as a distinct outlier, while most countries were merged into a single large, elongated cluster. This reflects a known limitation of single linkage; it is prone to "chaining," where clusters grow by progressively adding the closest individual points, leading to less meaningful groupings.

### Number of Groups

```{r, echo = FALSE}
source("https://raw.githubusercontent.com/jreuning/sds363_code/refs/heads/main/HClusEval3.R.txt")
#Call the function
hclus_eval(wbnorm, dist_m = 'euclidean', clus_m = 'complete', plot_op = T, 
           print_num = 15)
```
To determine the optimal number of clusters, we examined many metrics. The red line in the first plot represents the proportion of variance explained (RSQ), which increases sharply up to the fourth cluster, after which improvements taper off, indicating diminishing returns in explanatory power. The green line (semi-partial R-squared, SPRSQ) exhibits a clear elbow at the third cluster, suggesting a significant gain in cohesion when transitioning from two to three clusters, but smaller gains thereafter. The blue line (RMSSTD) begins to flatten after the fourth or fifth cluster, implying that additional clusters yield minimal improvements in within-group homogeneity.

The second plot, displaying cluster distances (CD), also reveals a visible elbow at around four clusters, with a less pronounced inflection near the sixth. Taking all these indicators into account, these patterns suggest that the most appropriate number of clusters is four, balancing between model simplicity and explanatory effectiveness.


### K-Means Clustering

```{r, echo = FALSE}
#kdata is just normalized input dataset
kdata <- wbnorm
n.lev <- 15  #set max value for number of clusters k

# Calculate the within groups sum of squared error (SSE) for the number of 
# solutions selected by the user
wss <- rnorm(10)
while (prod(wss==sort(wss,decreasing=T))==0) {
  wss <- (nrow(kdata)-1)*sum(apply(kdata,2,var))
  for (i in 2:n.lev) wss[i] <- sum(kmeans(kdata, centers=i)$withinss)}

# Calculate the within groups SSE for 250 randomized data sets 
# (based on the original input data)
k.rand <- function(x){
  km.rand <- matrix(sample(x),dim(x)[1],dim(x)[2])
  rand.wss <- as.matrix(dim(x)[1]-1)*sum(apply(km.rand,2,var))
  for (i in 2:n.lev) rand.wss[i] <- sum(kmeans(km.rand, centers=i)$withinss)
  rand.wss <- as.matrix(rand.wss)
  return(rand.wss)
}

rand.mat <- matrix(0,n.lev,250)

k.1 <- function(x) { 
  for (i in 1:250) {
    r.mat <- as.matrix(suppressWarnings(k.rand(kdata)))
    rand.mat[,i] <- r.mat}
  return(rand.mat)
}

# Same function as above for data with < 3 column variables
k.2.rand <- function(x){
  rand.mat <- matrix(0,n.lev,250)
  km.rand <- matrix(sample(x),dim(x)[1],dim(x)[2])
  rand.wss <- as.matrix(dim(x)[1]-1)*sum(apply(km.rand,2,var))
  for (i in 2:n.lev) rand.wss[i] <- sum(kmeans(km.rand, centers=i)$withinss)
  rand.wss <- as.matrix(rand.wss)
  return(rand.wss)
}

k.2 <- function(x){
  for (i in 1:250) {
    r.1 <- k.2.rand(kdata)
    rand.mat[,i] <- r.1}
  return(rand.mat)
}

# Determine if the data data table has > or < 3 variables and call appropriate 
# function above
if (dim(kdata)[2] == 2) { rand.mat <- k.2(kdata) } else { rand.mat <- k.1(kdata) }

# Plot within groups SSE against all tested cluster solutions for actual and 
# randomized data - 1st: Log scale, 2nd: Normal scale

xrange <- range(1:n.lev)
yrange <- range(log(rand.mat),log(wss))
plot(xrange,yrange, type='n', xlab='Cluster Solution', 
     ylab='Log of Within Group SSE', main='Cluster Solutions against Log of SSE')
for (i in 1:250) lines(log(rand.mat[,i]),type='l',col='red')
lines(log(wss), type="b", col='blue')
legend('topright',c('Actual Data', '250 Random Runs'), col=c('blue', 'red'), lty=1)

yrange <- range(rand.mat,wss)
plot(xrange,yrange, type='n', xlab="Cluster Solution", 
     ylab="Within Groups SSE", main="Cluster Solutions against SSE")
for (i in 1:250) lines(rand.mat[,i],type='l',col='red')
lines(1:n.lev, wss, type="b", col='blue')
legend('topright',c('Actual Data', '250 Random Runs'), col=c('blue', 'red'), lty=1)

# Calculate the mean and standard deviation of difference 
# between SSE of actual data and SSE of 250 randomized datasets
r.sse <- matrix(0,dim(rand.mat)[1],dim(rand.mat)[2])
wss.1 <- as.matrix(wss)
for (i in 1:dim(r.sse)[2]) {
  r.temp <- abs(rand.mat[,i]-wss.1[,1])
  r.sse[,i] <- r.temp}
r.sse.m <- apply(r.sse,1,mean)
r.sse.sd <- apply(r.sse,1,sd)
r.sse.plus <- r.sse.m + r.sse.sd
r.sse.min <- r.sse.m - r.sse.sd

# Plot differeince between actual SSE mean SSE from 250 randomized datasets - 
# 1st: Log scale, 2nd: Normal scale 

xrange <- range(1:n.lev)
if (min(r.sse.min) < 0){
   yrange <- range(log(r.sse.plus - min(r.sse.min)*1.05), log(r.sse.min - min(r.sse.min)*1.05))
} else {
   yrange <- range(log(r.sse.plus), log(r.sse.min))
}

plot(xrange,yrange, type='n',xlab='Cluster Solution', 
     ylab='Log of SSE - Random SSE', 
     main='Cluster Solustions against (Log of SSE - Random SSE)')
lines(log(r.sse.m), type="b", col='blue')
lines(log(r.sse.plus), type='l', col='red')
lines(log(r.sse.min), type='l', col='red')
legend('topright',c('SSE - random SSE', 'SD of SSE-random SSE'), col=c('blue', 'red'), lty=1)

xrange <- range(1:n.lev)
yrange <- range(r.sse.plus,r.sse.min)
plot(xrange,yrange, type='n',
     xlab='Cluster Solution', ylab='SSE - Random SSE', 
     main='Cluster Solutions against (SSE - Random SSE)')
lines(r.sse.m, type="b", col='blue')
lines(r.sse.plus, type='l', col='red')
lines(r.sse.min, type='l', col='red')
legend('topright',c('SSE - random SSE', 'SD of SSE-random SSE'), 
       col = c('blue', 'red'), lty = 1)

# Ask for user input - Select the appropriate number of clusters
#choose.clust <- function(){readline("What clustering solution would you like to use? ")} 
#clust.level <- as.integer(choose.clust())
clust.level <- 4

# Apply K-means cluster solutions - append clusters to CSV file
fit <- kmeans(kdata, clust.level)
aggregate(kdata, by=list(fit$cluster), FUN=mean)
clust.out <- fit$cluster
kclust <- as.matrix(clust.out)
kclust.out <- cbind(kclust, wbnorm)
write.table(kclust.out, file="kmeans_out.csv", sep=",")

# Display Principal Components plot of data with clusters identified

clusplot(kdata, fit$cluster, shade = F, labels = 2, lines = 0, color = T,
         lty = 4, main = 'Principal Components plot showing K-means clusters')


#Make plot of five cluster solution in space desginated by first two
#  two discriminant functions

plotcluster(kdata, fit$cluster, main="Four Cluster Solution in DA Space",
            xlab="First Discriminant Function", ylab="Second Discriminant Function")

# end of script
```

After performing K-means clustering, we generated a plot of the total sum of squares versus the number of clusters (k). The elbow in the curve appears around the fourth cluster, suggesting that four is an appropriate number of groups to retain.

Additionally, we included a plot of the sum of squared errors (SSE) for random permutations of the data (in red), which also shows a subtle kink around the fourth cluster. A plot of the log-transformed SSE similarly points to a maximum around the fourth index

Taken together, these plots indicate that four clusters best capture the underlying structure of the data, which aligns with our findings from hierarchical clustering, further strengthening the presence of four naturally distinct groupings within the dataset.

To visualize these groupings, we plotted the K-means clusters in principal component space (using the first and second components), as well as in discriminant analysis space, both of which show clear separation among the identified clusters.

### Plots in DA and PCA Space

```{r, echo = FALSE}
clust1 <- hclust(dist1, method = "ward.D")
cuts <- cutree(clust1, k = 4)
clusplot(wbnorm, cuts, color = TRUE, shade = TRUE, labels = 2, lines = 0, 
         main = "World Bank Five Cluster Plot, Complete Linkage, First two PC", 
         cex = .5)
plotcluster(wbnorm, cuts, main = "Five Cluster Solution in DA Space", 
            xlab = "First Discriminant Function", 
            ylab = "Second Discriminant Function", cex = .8)
```
The cluster plots in both discriminant analysis (DA) space and principal component analysis (PCA) space, based on Euclidean distance and Ward’s method, show a clear separation among five distinct groups. While our earlier metrics (such as SSE and RSQ) suggested four clusters as optimal, these visualizations provide additional justification for considering a fifth cluster, as it separates a subgroup more clearly than in previous methods. Together, these plots offer strong visual support for the underlying structure revealed through hierarchical clustering.


```{r, echo = FALSE, result="hide"}
for (i in 1:4){
  print(paste("Countries in Cluster ", i))
  print(rownames(wbnorm)[cuts == i])
  print (" ")
}

pca <- prcomp(wbnorm, scale = TRUE)
pca$rotation  # This shows the loadings

```

#### **Cluster Memberships

**Cluster 1**: Angola, Burundi, Benin, Burkina Faso, Central African Republic, Ivory Coast, Cameroon, The Democratic Republic of the Congo, Congo, Ethiopia, Ghana, Guinea, Gambia, Guinea-Bissau, Haiti, Kenya, Cambodia, Lao, Liberia, Lesotho, Madagascar, Mali, Mozambique, Mauritania, Malawi, Namibia, Niger, Nigeria, Nepal, Papua New Guinea, Rwanda, Sudan, Senegal, Sierra Leone, South Sudan, Eswatini, Chad, Togo, Tanzania, Uganda, Yemen, Zambia, Zimbabwe

**Cluster 2**: Albania, Argentina, Armenia, Azerbaijan, Bulgaria, Barbados, Chile, China, Costa Rica, Georgia, Croatia, Hungary, Kazakhstan, North Macedonia, Montenegro, Mongolia, Mauritius, Panama, Romania, Russian Federation, Serbia, Slovakia, Thailand, Ukraine, Uruguay

**Cluster 3**: Australia, Austria, Belgium, Belarus, Canada, Switzerland, Cyprus, Czechia, Germany, Denmark, Spain, Estonia, Finland, France, United Kingdom, Greece, Ireland, Iceland, Israel, Italy, Japan, South Korea, Lithuania, Luxembourg, Latvia, Malta, Netherlands, Norway, New Zealand, Poland, Portugal, Singapore, Slovenia, Sweden, United States

**Cluster 4**: Bangladesh, Bosnia and Herzegovina, Belize, Bolivia, Brazil, Bhutan, Colombia, Dominican Republic, Algeria, Ecuador, Egypt, Gabon, Guatemala, Guyana, Honduras, Indonesia, India, Iran, Iraq, Jamaica, Jordan, Kyrgyzstan, Saint Lucia, Sri Lanka, Morocco, Moldova, Maldives, Mexico, Nicaragua, Oman, Pakistan, Peru, Philippines, Paraguay, El Salvador, Sao Tome and Principe, Suriname, Tajikistan, Turkmenistan, Timor-Leste, Tonga, Tunisia, Turkey, Venezuela, Viet Nam, Samoa, South Africa


#### **PCA Loadings Table**

| Variable                        | PC1   | PC2   | PC3   | PC4   | PC5   | PC6   | PC7   | PC8   | PC9   | PC10  | PC11  |
|-------------------------------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|
| HDI_2021                      | 0.361 | -0.024| 0.050 | -0.085| 0.060 | -0.045| 0.328 | 0.106 | -0.142| 0.036 | -0.845|
| Life_Expectancy_2021         | 0.335 | -0.009| 0.254 | 0.062 | -0.504| -0.015| 0.551 | -0.151| 0.401 | -0.146| 0.239 |
| Mean_Years_Schooling_2021    | 0.337 | -0.017| -0.099| -0.147| 0.476 | -0.616| 0.068 | 0.345 | 0.236 | -0.094| 0.246 |
| GNI_Per_Capita_2021          | 0.352 | -0.008| 0.045 | -0.131| 0.137 | 0.290 | 0.296 | 0.097 | -0.689| 0.123 | 0.408 |
| Gender_Dev_Index_2021        | 0.217 | -0.396| -0.622| -0.359| -0.461| 0.009 | -0.188| 0.132 | 0.024 | 0.119 | 0.015 |
| Gender_Inequality_Index_2021| -0.347| 0.128 | -0.014| -0.246| 0.065 | 0.108 | 0.365 | 0.259 | 0.266 | 0.718 | 0.031 |
| Maternal_Mortality_Rate_2021| -0.349| -0.024| -0.098| -0.150| 0.010 | 0.311 | 0.257 | 0.554 | 0.045 | -0.611| -0.019|
| Adolescent_Birth_Rate_2021  | -0.326| -0.033| -0.228| -0.447| 0.145 | -0.246| 0.363 | -0.608| -0.138| -0.198| -0.004|
| Female_Labour_Force_2021    | -0.050| -0.722| -0.152| 0.456 | 0.345 | 0.183 | 0.236 | -0.118| 0.138 | 0.070 | 0.015 |
| Male_Labour_Force_2021      | -0.121| -0.544| 0.670 | -0.422| -0.066| -0.113| -0.187| 0.081 | -0.060| 0.018 | -0.005|
| CO2_Emissions_percapita_2021| 0.329 | 0.089 | 0.030 | -0.390| 0.369 | 0.565 | -0.201| -0.236| 0.417 | -0.075| -0.006|

___

Cluster 1 (e.g. Uganda, Nigeria, Sudan) consists of countries with low Human Development Index (HDI) indicators. These countries are positioned on the far left of the first principal component, which appears to represent a general measure of overall HDI performance. In contrast, Cluster 3 (e.g., Denmark, Finland, Japan) includes highly developed countries with strong HDI metrics, scoring highest along the same component. The remaining two clusters are less clear. Cluster 2, which includes countries like China, seems to represent nations in transition—those currently progressing toward developed status—positioned between the low and high extremes along the first principal component. Cluster 4 appears to contain less developed countries that vary more along the second principal component, which seems to capture gender-related development factors, given the strong loadings of variables like male/female labor force participation and the Gender Development Index.

Overall, the clustering results fit the context of our dataset, effectively grouping countries by their development stage, from low HDI to highly developed, with transitional and intermediate categories in between.

Building on the insights from our cluster analysis, we now apply discriminant analysis to formally assess how well countries can be classified into the identified clusters based on key development indicators.

***Discriminant Analysis***

```{r, warning = FALSE, echo = FALSE}
library(MASS)
library(biotools)
library(klaR)
library(car)
library(dplyr)
library(lubridate)
library(ggplot2)
library(ggExtra)
library(heplots)
```

```{r, echo = FALSE}
data <- data[complete.cases(data) & data$Region != "", ]
```

### Analysis of Multivariate Normality and Similar Covariances Matrices

#### Chi-Square Quantile Plots 

We will first create **Chi-Square quantile plots** to evaluate multivariate normality within each group. 

```{r, echo = FALSE}
cqplot <- function(data1, main) {
  library(car)  # Load the car package for qqPlot function
  
  center <- colMeans(data1)  
  cov_matrix <- cov(data1)   
  
  # calculate mahalanobis distances
  mahalanobis_dist <- mahalanobis(data1, center, cov_matrix)
  
  theoretical_quantiles <- qchisq(ppoints(length(mahalanobis_dist)), df = ncol(data1))
  
  qqPlot(mahalanobis_dist, distribution = "chisq", df = ncol(data1),
         main = main,
         xlab = "Theoretical Quantiles",
         ylab = "Observed Mahalanobis Distances")
}

columns <- c(4, 5, 7, 11, 15)

# create a plot for each group 
par(mfrow = c(1,2), pty = "s", cex = 0.8)
cqplot(data[data$Region == "AS", columns], main = "Asia (AS)")
cqplot(data[data$Region == "EAP", columns], main = "East Asia & Pacific (EAP)")
cqplot(data[data$Region == "ECA", columns], main = "Europe & Central Asia (ECA)")
cqplot(data[data$Region == "LAC", columns], main = "Latin America & the Caribbean (LAC)")
cqplot(data[data$Region == "SA", columns], main = "South Asia (SA)")
cqplot(data[data$Region == "SSA", columns], main = "Sub-Saharan Africa (SSA)")
par(mfrow = c(1,1))

```
Based on the chi-square quantile plots for each region (AS, EAP, ECA, LAC, SA, and SSA), the data roughly follows a linear trend, indicating the assumption of multivariate normality holds across all groups. Most observed Mahalanobis distances align well with the theoretical quantities and remain within the confidence bounds.

However, there does appear to be more deviations from normality at upper quartiles, such as in LAC and ECA, while some points slightly exceed the theoretical line. While these deviations suggest the presence of outliers or skewedness, they are still within the acceptable limits, so the normality assumption is not strongly violated.

#### Covariances Matrices - Box's M and Matrices

We then create covariances matrices to see similarity and use the Box's M statistic. The Box's M is used to test equality of entire covariances matrices as equal covariance matrices are an assumption of discriminant analysis.

```{r, echo = FALSE}

library(heplots)
# covariance matrices for all the regions 
print("Covariance Matrix for AS")
cov_as <- cov(data[data$Region == "AS", columns])
print(cov_as)

print("Covariance Matrix for EAP")
cov_eap <- cov(data[data$Region == "EAP", columns])
print(cov_eap)

print("Covariance Matrix for ECA")
cov_eca <- cov(data[data$Region == "ECA", columns])
print(cov_eca)

print("Covariance Matrix for LAC")
cov_lac <- cov(data[data$Region == "LAC", columns])
print(cov_lac)

print("Covariance Matrix for SA")
cov_sa <- cov(data[data$Region == "SA", columns])
print(cov_sa)

print("Covariance Matrix for SSA")
cov_ssa <- cov(data[data$Region == "SSA", columns])
print(cov_ssa)

# ratios
print("Ratio of Largest to Smallest Covariance Elements for AS vs EAP")
cov_rat_as_eap <- cov_as / cov_eap
cov_rat_as_eap[abs(cov_rat_as_eap) < 1] <- 1 / 
  (cov_rat_as_eap[abs(cov_rat_as_eap) < 1])
print(round(cov_rat_as_eap, 1))

print("Ratio of Largest to Smallest Covariance Elements for ECA vs LAC")
cov_rat_eca_lac <- cov_eca / cov_lac
cov_rat_eca_lac[abs(cov_rat_eca_lac) < 1] <- 1 / 
  (cov_rat_eca_lac[abs(cov_rat_eca_lac) < 1])
print(round(cov_rat_eca_lac, 1))

print("Ratio of Largest to Smallest Covariance Elements for SA vs SSA")
cov_rat_sa_ssa <- cov_sa / cov_ssa
cov_rat_sa_ssa[abs(cov_rat_sa_ssa) < 1] <- 1 / 
  (cov_rat_sa_ssa[abs(cov_rat_sa_ssa) < 1])
print(round(cov_rat_sa_ssa, 1))

# Box M statistic 
print("Box's M statistic for all regions")
boxM_result <- boxM(data[, columns], data$Region)
print(boxM_result)

```
*Looking at our covariance matrices and the ratio of the largest to smallest elements of these entries, our covariance matrices seem to be pretty similar considering the ratio of our entries are mostly all less than 4. However, Box's M-test gave us a p-value of 0.0002912, which suggests that are covariance matrices are statistically significantly different. This is likely dude to our relatively large sample size (over 80 observations) and high degrees of freedom (df = 75), making the test highly sensitive to small deviations.*

#### Covariances Matrices - Testing Out Transformations

```{r, echo = FALSE}
library(heplots)

data_log_transformed <- data.frame(data) # Creates a deep copy

log_transform_columns <- c("HDI_2021", "Life_Expectancy_2021", "GNI_Per_Capita_2021", 
                           "Gender_Inequality_Index_2021", "Male_Labour_Force_2021")

print("Raw Log Determinants (Before Log Transformation)")
log_dets_before <- c()
for (region in unique(data$Region)) {
  cov_matrix <- cov(data_log_transformed[data$Region == region, log_transform_columns])
  log_det <- log(det(cov_matrix))
  log_dets_before <- c(log_dets_before, log_det)
  cat(region, ":", log_det, "\n")
}

data_log_transformed[log_transform_columns] <- 
  log(data_log_transformed[log_transform_columns] + 0.001)


print("Covariance Matrices After Log Transformation")
cov_matrices <- list()
for (region in unique(data$Region)) {
  cov_matrices[[region]] <- cov(data_log_transformed[data_log_transformed$Region
                                                     == region, log_transform_columns])
}

print("Raw Log Determinants (After Log Transformation)")
log_dets_after <- c()
for (region in unique(data$Region)) {
  log_det <- log(det(cov_matrices[[region]]))
  log_dets_after <- c(log_dets_after, log_det)
  cat(region, ":", log_det, "\n")
}

log_det_differences <- max(log_dets_after) - min(log_dets_after)
print(paste("Max-Min Difference in Log Determinants (After Log Transformation):", 
            log_det_differences))

print("Box's M statistic for all regions")
boxM_result <- boxM(data_log_transformed[, log_transform_columns], 
                    data_log_transformed$Region)
print(boxM_result)

# Compare sensitivity
if (log_det_differences < 1) {
  print("Log determinants are nearly equal. Box's M may be overly sensitive.")
} else {
  print("Significant differences in log determinants.")
}

```
*The covariance matrices exhibit large ratios, suggesting a violation of multivariate normality. Applying log transformations did not fully correct the issue, as significant differences remain in the log determinants across groups, with a maximum-minimum difference of 8.37.*

*Additionally, Box’s M-test remains highly significant (p-value = 2.456e-13), reinforcing that the covariance matrices are statistically different. While Box’s M-test is known to be sensitive to large sample sizes, the large spread in log determinants suggests that this result reflects true differences rather than mere statistical sensitivity.*

*Since LDA assumes similarity of covariance matrices, it may not be suitable in this case. Given these results, QDA may be a better alternative as it does not assume equal covariances.*

#### Matrix Plots

To determine what our data looks like with two variables at a time. 

```{r, echo = FALSE}

region_pairs <- list(
  c("AS", "EAP"),
  c("ECA", "LAC"),
  c("SA", "SSA")
)

for (pair in region_pairs) {
  
  filtered_data <- data[data$Region %in% pair, ]
  
  filtered_data$Region_Factor <- as.numeric(as.factor(filtered_data$Region))
  
  plot(filtered_data[, columns],
       col = filtered_data$Region_Factor + 2,  
       pch = filtered_data$Region_Factor + 15, 
       cex = 1.2,
       main = paste(pair[1], "vs", pair[2]))  
}
                          
```
*The matrix plots show that the selected variables serve as strong discriminators between the two groups. Many variables show a clear direction of deviation, which suggests stronger results when using discriminant analysis. However, there are some overlaps between the groups that are visible in certain pairs, which may impact performance. Based on these results, applying QDA should be good, and we will also use Wilks Lambda for best feature selection as well.*

### Discriminant Analysis

#### Linear Discriminant Analysis

```{r, echo = FALSE}
library(MASS)

hdi_lda <- lda(data[, log_transform_columns], grouping = data$Region)

ctraw <- table(data$Region, predict(hdi_lda)$class)
print("Confusion Matrix:")
print(ctraw)

lda_acc <- round(sum(diag(prop.table(ctraw))), 2)
print(paste("LDA Accuracy:", lda_acc))

```
*LDA was applied since our groups have relatively similar covariance structures.The overall accuracy of LDA when it comes to classification is approximately 0.72 for the given variables, showing that the model was performing reasonably well. However, misclassifications are prevalent, particularly for AS (4/9 = 44%), EAP (1/13 = 8%), and SA (2/8 = 25%), which suggests overlapping feature distributions among these regions. Regions like ECA (14/16 = 87.5%), LAC (21/25 = 84%), and SSA (38/40 = 95%) had high accuracy. However, Box's M test showed that our covariance matrices are statistically different, suggesting that linear discriminant analysis' assumption of equal covariances does not hold. Quadratic discriminant analysis may be more fitting given that it allows for different covariance matrices.*

#### Quadratic Discrminant Analysis 

We decided to use Quadratic Discrminant analysis as it provides a way to determine group means for each variable and understand how different regions are separated based on their feature distributions. 

```{r, echo = FALSE}
(hdi.disc <- qda(data[, columns], grouping = data$Region))

ctrawQ <- table(data$Region, predict(hdi.disc)$class)
round(sum(diag(prop.table(ctrawQ))),2)
```

*The prediction accuracy of QDA is 0.84, which is higher than the prediction accuracy of linear discriminant analysis of 0.72. The improvement suggests that quadratic discriminant analysis is more appropriate as it does not assume equal covariance matrices across groups. As mentioned earlier, the Box's M test showed significant differences in covariance matrices so QDA is most likely capturing the true variance patterns more effectively than LDA. While the accuracy gain is marginally better (0.12), QDA having the higher prediction accuracy and its ability to handle differing covariance structures makes it the better choice.*

#### Stepwise Discriminant Analysis 

Stepwise Linear Discriminant Analysis

```{r, echo = FALSE}
library(klaR)  # Load klaR package for stepclass()

stepwise_lda <- stepclass(Region ~ HDI_2021 + Life_Expectancy_2021 + 
                            GNI_Per_Capita_2021 + 
                           Gender_Inequality_Index_2021 + 
                            Male_Labour_Force_2021,
                           data = data, method = "lda", direction = "both", 
                          fold = nrow(data))

stepwise_lda
stepwise_lda$result.pm


```

*Using stepwise LDA, gender inequality and life expectancy were the key discriminators and the classification accuracy was 63.96%, still lower than QDA's accuracy of 84%. Because LDA assumes equal covariance matrices and Box's M test confirmed significant covariance differences, the assumptions of LDA may not hold, so LDA would not be a good choice.* 

We will now use quadratic discriminant analysis. 

```{r, echo = FALSE}
library(klaR)  # Load klaR package for stepclass()

stepwise_qda <- stepclass(Region ~ HDI_2021 + Life_Expectancy_2021 + 
                            GNI_Per_Capita_2021 + 
                           Gender_Inequality_Index_2021 + 
                            Male_Labour_Force_2021,
                           data = data, method = "qda", direction = "both", 
                          fold = nrow(data))

stepwise_qda

data$Region <- as.factor(data$Region)
partimat(Region ~ Life_Expectancy_2021 + Gender_Inequality_Index_2021,
         data = data, method = "qda", main = "QDA Partition Plot")


```

*Using stepwise QDA, we identified the key discriminators being Gender Inequality Index and Life Expectancy for classifying regions. However, the final classification accuracy of stepwise QDA is 63.06%, which is lower than the full QDA's model's accuracy of 84%, suggesting that while these two groups contribute significantly to group separation, removing the other predictors may resulted in information loss, leading to decreased classification performance. Additionally, the apparent error rate of QDA is lower than LDA, showing that QDA is better at separating classes. Since QDA does not assume equal covariance matrices, our data is not multivariate normal, and Box's M test confirmed covariance differences, QDA remains the best model choice over LDA.*

### Wilk's Lambda Test

```{r, echo = FALSE}
data.manova <- manova(as.matrix(data[, columns]) ~ data$Region)
summary.manova(data.manova, test = "Wilks")
summary.aov(data.manova)
```

*A Wilks' lambda of approximately 0.14 is relatively small and is statistically significant given a p-value of 2.2e-16, which is less than our alpha of 0.05, and suggests that there is a statistically significant difference in the multivariate means between the regions. The strongest discriminators are Life Expectancy (p value 2.2e-16 and F-Statistic 34.7) and Gender Inequality Index (p value 2.2e-16 and F-Statistic 29.87), while Male Labor Force Participation is the weakest predictor. Since Wilks' Lamda is closer to 0, it indicates a strong group separation.*

### Discriminant Functions Significance

```{r, echo = FALSE}
lda_scores <- predict(hdi_lda)$x 

# MANOVA with all 5 discriminant functions
lda_manova_5 <- manova(lda_scores ~ data$Region)
summary(lda_manova_5, test = "Wilks")  

# MANOVA with only the last 4 discriminant functions
lda_manova_4 <- manova(lda_scores[, 2:5] ~ data$Region)
summary(lda_manova_4, test = "Wilks") 

# MANOVA with only the last 3 discriminant functions
lda_manova_3 <- manova(lda_scores[, 3:5] ~ data$Region)
summary(lda_manova_3, test = "Wilks")  

hdi_lda
```

*Of our 5 discriminant functions, the first and second discriminant function are the only ones that were statistically significant, meaning they play a significant role in distinguishing between the groups. To determine this, we conducted a stepwise Wilks' Lamda test, progressively assessing the significance of all five discriminant functions. First, we tested the significance of all five discriminant functions, which yielded a Wilks' Lambda of 0.146 and a p-value of <2.2e-16, which showed significant separation between groups. Then we proceeded to test 4 of the discriminant functions (excluding the one with the most explanatory power), which was also statistically significant with a p-value of 2.16e-11, indicating that at least the first two discriminant functions were contributing meaningfully.*

*However, when we further tested only the last three discriminant functions, the p-value was 0.8185, which is higher than our alpha of 0.05. This means we fail to reject the null hypothesis that at least one discriminant function is significant and all these three functions do not significantly improve group separation. As noted, it is evident that LD1 and LD2 are the primary drivers of group separation, whereas the last three functions add little explanatory power. This shows that reducing the model to just these two significant functions could simplify interpretation without compromising classification accuracy.*

### Regular & Leave-One-Out Classification

```{r, echo = FALSE}
# regular QDA Classification
ctrawQ <- table(data$Region, predict(hdi.disc)$class)
ctrawQ
round(sum(diag(prop.table(ctrawQ))),2)

# Leave-One-Out Cross-Validation QDA Classification
qda_cv_pred <- qda(data[, columns], grouping = data$Region, CV = TRUE)
ctCVQ <- table(data$Region, qda_cv_pred$class)
ctCVQ
round(sum(diag(prop.table(ctCVQ))),2)
```
*The raw classification accuracy was 84%, while the cross-validation accuracy was 62%, indicating a decrease in predictive performance when tested on unseen data. The model performs well on the training data but struggles with generalization, which suggests overfitting, where the model captures noise rather than the true patterns in the data.*

*When analyzing the confusion matrices, SSA had the highest classification accuracy, as most of its observations were correctly classified in both raw and cross-validated results (38 out of 41 in raw and 34 out of 41 in LOOCV). In contrast, regions such as AS, EAP, and LAC had a higher number of misclassifications. Additionally, EAP, LAC, and SA experienced significant accuracy losses, further supporting the overfitting hypothesis.*

*Moreover, the 22% drop in accuracy from the raw method to cross-validation suggests that the latter may be too complex for this dataset, leading to overfitting.*

### Standardized Discriminant Coefficients

```{r, echo = FALSE}
print("Raw (Unstandardized) Coefficients")
round(hdi_lda$scaling,2)

print("Normalized Coefficients")
round(hdi_lda$scaling/sqrt(sum(hdi_lda$scaling^2)),2)

print("Standardized Coefficients")
hdi_lda_standardized <- lda(scale(data[, columns]), grouping = data$Region)
print(round(hdi_lda_standardized$scaling, 2))
```
*Analyzing our standardized coefficients, HDI, Life expectancy, and Gender Inequality appear to be our stronger discriminators while GNI per Capita (GNIpc) and Male Labor Force have weaker contributions. This aligns with our univariate comparisons, where although GNIpc and Male Labor Force were statistically significant, their F-values were lower compared to the other three variables, indicating a weaker impact on group differentiation.*

*When looking at the magnitude of the coefficients for our discriminant functions, life expectancy had the largest coefficient overall in LD1, as well as large coefficients in LD2 and LD3. HDI has relatively large coefficients in LD3 and LD5, and life expectancy has large coefficients and strong influence in LD2 and LD3. These results show that socioeconomic and human development indicators play a greater role in distinguishing regions than economic metrics such as GNI per Capita or labor force participation.*

### Score Plots

The LDA Score Plot reveals distinct separation patterns among the regions based on the first two discriminant functions (LD1 and LD2).

```{r, echo = FALSE}
lda_scores <- predict(hdi_lda)$x

# Extract unique region names for plotting
region_names <- unique(data$Region)

# Generate the score plot for LD1 vs LD2
plot(lda_scores[,1], lda_scores[,2], type = "n",
     main = "LDA Score Plot for HDI Data",
     xlab = "LDA Axis 1", ylab = "LDA Axis 2")

# Loop through each region to plot points with different colors and symbols
for (i in 1:length(region_names)) {
  points(lda_scores[data$Region == region_names[i], 1], 
         lda_scores[data$Region == region_names[i], 2], 
         col = i + 1, pch = 15 + i, cex = 1.2)
}

# Add a legend to distinguish groups
legend("topright", legend = region_names, 
       col = c(2:(length(region_names) + 1)), 
       pch = c(15:(15 + length(region_names))))
```

*Countries in Sub Saharan Africa (SSA) stands out significantly along LD1 (x-axis), suggesting that this region is well-differentiated from others. On the other hand, Europe and Central Asia (ECA) forms a distinct cluster in the bottom left corner, primarily separated along LD2 (y-axis), which indicates that the ECA is differentiated from other regions primarily due to variations in Life Expectancy and Gender Inequality Index.*

*The other four regions-East Asia & Pacific (EAP), South Asia (SA), Americas (AS), and Latin America & Caribbean (LAC)-have considerable overlap. This suggests that these regions share similar features concerning LD1 and LD2, making it more difficult to distinguish them based on these two dimensions alone, meaning that variation exists and these regions may have similar economic, social, or demographic structures in relation to the predictor variables.*

*Additionally, when looking at LD1, SSA appears to separate itself primarily based on HDI, Life Expectancy, and the Gender Inequality Index, while ECA differentiates itself along LD2, driven largely by Life Expectancy and Gender Inequality. Furthermore, there are outliers, such as in the Americas, where some observations deviate from the main cluster, suggesting that certain countries in this group show unique features that differ from the general trends of their respective regions.*

### LDA Partition Plot

```{r, echo = FALSE}
library(klaR)
data$Region <- as.factor(data$Region)
    
partimat(Region ~ Life_Expectancy_2021 + Gender_Inequality_Index_2021, 
         data = data, method = "lda", main = "LDA Partition Plot")

```

*Looking at the partition plot, we can see more clearly how countries in SSA and ECA distinguish themselves, while the remaining regions overlap significantly, suggesting that they are less differentiated based on these two variables. This validates our conclusions from the previous analysis, as Gender Inequality and Life Expectancy were identified as significant contributors to both LD1 and LD2. SSA primarily occupies the lower right portion of the plot, whereas ECA is concentrated in the upper left. The substantial overlap among the other regions, particularly LAC, EAP, and AS, indicates that additional variables may be needed to improve classification accuracy between these groups.*

### K-Nearest Neighbors

K-Nearest Neighbors (KNN) classifies data points based on the majority class of their closest k neighbors, with k=5 in this case. 

```{r, echo = FALSE}

library(class)

# Define training data
train_X <- data[, c("HDI_2021", "Life_Expectancy_2021")]
train_y <- as.factor(data$Region)

# Generate a grid of points for decision boundary
x_range <- seq(min(train_X[,1]), max(train_X[,1]), length.out = 100)
y_range <- seq(min(train_X[,2]), max(train_X[,2]), length.out = 100)
grid <- expand.grid(HDI_2021 = x_range, Life_Expectancy_2021 = y_range)

# Perform KNN classification
knn_pred <- knn(train = train_X, test = grid, cl = train_y, k = 5)

# Convert predictions to a data frame for plotting
grid$Region <- knn_pred

# Create decision boundary plot
ggplot(data, aes(x = HDI_2021, y = Life_Expectancy_2021, color = Region)) +
  geom_point() +
  geom_tile(data = grid, aes(fill = Region), alpha = 0.3) +
  theme_minimal() +
  labs(title = "KNN Classification (k = 5)", x = "HDI_2021", y = "Life Expectancy")


```

*In the plot, the color of each dot corresponds to the actual classification of the region, while the background grid represents the predicted classification for different areas of the feature space. Based on the plot, SSA is clearly concentrated in the lower left corner of the plot, indicating that countries with lower Human Development Index (HDI) and life expectancy are predominantly classified as SSA. This aligns with our prior analyses showing that SSA tends to have lower values for these indicators compared to other regions.*

*For other regions, there is considerable overlap, particularly among East Asia & Pacific (EAP), Europe & Central Asian (ECA), Latin America & the Caribbean (LAC), and South Asia (SA), suggesting that HDI and Life Expectancy alone may not be sufficient enough to classify these groups. The decision bands are horizontal, suggesting that life expectancy plays a more pivotal role in classification relative to HDI when determining the region assignment.*


**Conclusion**

Based on our analysis, we uncovered some interesting findings relating to some of the underlying trends/patterns in our dataset based on the economic, social and environmental benchmarks of different countries. Through PCA, we settled on using two principle components to explain a significant portion of the variability in our data (roughly 78%): the first component was a general measure of overall well-being and development while the second component was a measurement of a country's gender inequality. In addition, we visualized our data, using these two components, and identified two outliers (based on a 95% confidence interval ellipse) to be Yemen and Madagascar. Next, we used discriminant analysis to unpack whether there existed natural clusters underlying our data. From using K-means clustering to Ward's method (with Euclidean distance), we identified 4 distinct clusters: the leftmost cluster distinguished itself as the group of countries with overall low HDI indicators while the rightmost cluster was composed of more developed countries with contradistinction high metrics of well-being. The other two clusters were more nuanced, but further analysis showed that one of these clusters appeared to be made up of countries that were transitioning to be more developed (such as China) while the other cluster was made of countries that were less developed and more varied along the second axis, indicating greater levels of inequality. For our final method, we were interested in determining whether a pre-defined grouping - namely, the geographic region - was an adequate discriminator based on developmental metrics. Using discriminant analysis, we found that Sub-Saharan Africa and Europe & Central Asia were well-differentiated from the other groups based on indicators relating to the general development of a country (life expectancy, GNIpc) and the overall inequality, while the other four regions were more clustered in the center suggesting they shared more commonality among those indicators which made it more difficult to distinguish based on those two discriminators alone. 

Overall, we were successful in tackling the aim of our project to uncover patterns in development, inequality, and regional trends across countries through the use of multivariate analysis. We recognize that there are limitations and further extensions that can be done in future analysis. For instance, our methods captured major trends and patterns based on holistic measures of a country's development, more country specific factors were not captured by our models and neither were more qualitative metrics that could have had significant impacts. In future iterations of this project, we could not only incorporate further data - other metrics such as more environmental risk indicators or policy intervention / regulatory measures - but also incorporate time dynamics to visualize changes over time. Nonetheless, our project allowed us to uncover the complex interplay between economic and social factors that shape development outcomes.
